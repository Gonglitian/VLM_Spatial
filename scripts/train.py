# train.py
import hydra
import torch
import wandb
import os
import random
import numpy as np
from omegaconf import DictConfig, OmegaConf
from unsloth import FastVisionModel, is_bf16_supported
from datasets import load_dataset
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig
from tqdm import tqdm


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def filter_none_values(config_dict):
    """è¿‡æ»¤æ‰Noneå€¼çš„é…ç½®é¡¹"""
    return {k: v for k, v in config_dict.items() if v is not None}


def convert_to_conversation(sample):
    """å°†æ•°æ®é›†æ ·æœ¬è½¬æ¢ä¸ºå¯¹è¯æ ¼å¼ï¼ˆé€‚é…question+answeræ ¼å¼ï¼‰"""
    conversation = [
        { "role": "user",
          "content" : [
            {"type" : "text",  "text"  : sample["question"]},
            {"type" : "image", "image" : sample["image"]} ]
        },
        { "role" : "assistant",
          "content" : [
            {"type" : "text",  "text"  : sample["answer"]} ]
        },
    ]
    return { "messages" : conversation }


@hydra.main(version_base="1.1", config_path="../conf", config_name="train_qwen")
def main(cfg: DictConfig):
    print("=== ğŸ”§ é…ç½®ä¿¡æ¯ ===")
    print(OmegaConf.to_yaml(cfg))
    
    # ======================================================================
    # ğŸƒ è¿è¡Œç¯å¢ƒè®¾ç½®
    # ======================================================================
    if hasattr(cfg.run, 'cuda_visible_devices'):
        os.environ["CUDA_VISIBLE_DEVICES"] = str(cfg.run.cuda_visible_devices)
        print(f"ğŸ”§ è®¾ç½®CUDAè®¾å¤‡: {cfg.run.cuda_visible_devices}")
    
    if hasattr(cfg.run, 'seed') and cfg.run.seed is not None:
        set_seed(cfg.run.seed)
        print(f"ğŸ² è®¾ç½®éšæœºç§å­: {cfg.run.seed}")
    
    # ======================================================================
    # ğŸ“ˆ W&Båˆå§‹åŒ–
    # ======================================================================
    if cfg.wandb and cfg.train.report_to and "wandb" in cfg.train.report_to:
        wandb_config = filter_none_values({
            "project": cfg.wandb.project,
            "name": cfg.wandb.run_name,
            "entity": getattr(cfg.wandb, 'entity', None),
            "tags": getattr(cfg.wandb, 'tags', None),
            "notes": getattr(cfg.wandb, 'notes', None),
            "config": OmegaConf.to_container(cfg, resolve=True)
        })
        wandb.init(**wandb_config)
        print(f"ğŸ“ˆ W&Båˆå§‹åŒ–å®Œæˆ: {cfg.wandb.project}/{cfg.wandb.run_name}")
        
        # è®°å½•å…³é”®é…ç½®å‚æ•°ï¼Œä½¿å…¶åœ¨W&Bä¸­æ›´çªå‡ºæ˜¾ç¤º
        key_config = {
            "model_name": cfg.model.name,
            "lora_r": cfg.lora.r,
            "lora_alpha": cfg.lora.alpha,
            "batch_size": cfg.train.batch_size,
            "learning_rate": cfg.train.learning_rate,
            "num_epochs": cfg.train.num_epochs,
            "max_samples": cfg.data.max_samples,
            "dataset": cfg.data.dataset_name,
        }
        wandb.config.update(key_config)
        print("ğŸ“‹ å…³é”®é…ç½®å‚æ•°å·²è®°å½•åˆ°W&B")
    
    # ======================================================================
    # ğŸ¤– æ¨¡å‹åŠ è½½
    # ======================================================================
    print(f"ğŸš€ åŠ è½½æ¨¡å‹: {cfg.model.name}")
    
    # æç®€æ¨¡å‹åŠ è½½ - åªç”¨å¿…éœ€å‚æ•°
    model, tokenizer = FastVisionModel.from_pretrained(
        model_name=cfg.model.name,
        load_in_4bit=cfg.model.load_in_4bit
    )
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # ======================================================================
    # ğŸ¯ LoRAé…ç½® - æç®€åŒ–
    # ======================================================================
    print("ğŸ”§ é…ç½®LoRAå‚æ•°")
    
    # åŸºç¡€LoRAå‚æ•°
    lora_config = {
        "model": model,
        "r": cfg.lora.r,
        "lora_alpha": cfg.lora.alpha,
        "lora_dropout": cfg.lora.dropout,
        "finetune_vision_layers": cfg.lora.finetune_vision_layers,
        "finetune_language_layers": cfg.lora.finetune_language_layers,
        "finetune_attention_modules": cfg.lora.finetune_attention_modules,
        "finetune_mlp_modules": cfg.lora.finetune_mlp_modules,
    }
    
    # è‡ªåŠ¨æ·»åŠ å¯é€‰å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ä¸”ä¸ä¸ºNoneï¼‰
    optional_lora_params = ['bias', 'use_rslora', 'loftq_config']
    for param in optional_lora_params:
        if hasattr(cfg.lora, param):
            value = getattr(cfg.lora, param)
            if value is not None:
                lora_config[param] = value
    
    # ç‰¹æ®Šå¤„ç†target_modules
    if hasattr(cfg.lora, 'target_modules') and cfg.lora.target_modules:
        target_modules = cfg.lora.target_modules
        if isinstance(target_modules, str) and target_modules != "all-linear":
            lora_config["target_modules"] = [target_modules]
        elif isinstance(target_modules, (list, tuple)):
            lora_config["target_modules"] = target_modules
    
    model = FastVisionModel.get_peft_model(**lora_config)
    
    # ======================================================================
    # ğŸ“Š æ•°æ®åŠ è½½å’Œå¤„ç† - æç®€åŒ–
    # ======================================================================
    print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {cfg.data.dataset_name}")
    
    # åŠ è½½æ•°æ®é›†
    ds = load_dataset(
        path=cfg.data.dataset_name, 
        split=cfg.data.split,
        streaming=getattr(cfg.data, 'streaming', False)
    )
    
    # éªŒè¯é›†åˆ†å‰²
    eval_dataset = None
    if hasattr(cfg.data, 'val_split_ratio') and cfg.data.val_split_ratio > 0:
        split_dataset = ds.train_test_split(
            test_size=cfg.data.val_split_ratio, 
            seed=getattr(cfg.run, 'seed', None)
        )
        ds, eval_dataset = split_dataset['train'], split_dataset['test']
        print(f"ğŸ“Š è®­ç»ƒé›†: {len(ds)} æ ·æœ¬, éªŒè¯é›†: {len(eval_dataset)} æ ·æœ¬")
    
    # æ‰“ä¹±å’Œæˆªå–æ•°æ®
    if getattr(cfg.data, 'shuffle', False):
        ds = ds.shuffle(seed=getattr(cfg.run, 'seed', None))
    
    if cfg.data.max_samples and cfg.data.max_samples > 0:
        ds = ds.select(range(min(cfg.data.max_samples, len(ds))))
        if eval_dataset:
            eval_samples = min(int(cfg.data.max_samples * cfg.data.val_split_ratio), len(eval_dataset))
            eval_dataset = eval_dataset.select(range(eval_samples))
    
    print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒæ ·æœ¬æ•°: {len(ds)}")
    if eval_dataset:
        print(f"ğŸ“Š æœ€ç»ˆéªŒè¯æ ·æœ¬æ•°: {len(eval_dataset)}")
    
    # æ•°æ®æ ¼å¼è½¬æ¢ - ä½¿ç”¨æ¯ä¸ªæ ·æœ¬çš„questionä½œä¸ºæŒ‡ä»¤
    converted_dataset = [convert_to_conversation(sample) for sample in tqdm(ds, desc="ğŸ”„ è½¬æ¢è®­ç»ƒæ•°æ®")]
    eval_converted_dataset = None
    if eval_dataset:
        eval_converted_dataset = [convert_to_conversation(sample) for sample in tqdm(eval_dataset, desc="ğŸ”„ è½¬æ¢éªŒè¯æ•°æ®")]
    
    print("âœ… æ•°æ®è½¬æ¢å®Œæˆ!")
    
    # ======================================================================
    # ğŸ‹ï¸ è®­ç»ƒé…ç½® - æç®€åŒ–
    # ======================================================================
    print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ")
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    training_config = {
        "per_device_train_batch_size": cfg.train.batch_size,
        "gradient_accumulation_steps": cfg.train.grad_accum,
        "learning_rate": cfg.train.learning_rate,
        "bf16": cfg.train.bf16 and is_bf16_supported(),
        "fp16": cfg.train.fp16,
        "optim": cfg.train.optim,
        "output_dir": cfg.train.output_dir,
        "remove_unused_columns": False,
        "dataset_kwargs": {"skip_prepare_dataset": True},
        "max_seq_length": cfg.train.max_seq_length,
        "logging_steps": cfg.train.logging_steps,
        "save_strategy": cfg.train.save_strategy,
    }
    
    # è®­ç»ƒè½®æ•°æˆ–æ­¥æ•°
    if hasattr(cfg.train, 'max_steps') and cfg.train.max_steps:
        training_config["max_steps"] = cfg.train.max_steps
    else:
        training_config["num_train_epochs"] = cfg.train.num_epochs
    
    # è‡ªåŠ¨æ·»åŠ æ‰€æœ‰å¯é€‰è®­ç»ƒå‚æ•°
    optional_train_params = [
        'lr_scheduler_type', 'warmup_steps', 'weight_decay', 'gradient_checkpointing',
        'save_steps', 'dataloader_num_workers', 'disable_tqdm'
    ]
    for param in optional_train_params:
        if hasattr(cfg.train, param):
            value = getattr(cfg.train, param)
            if value is not None:
                training_config[param] = value
    
    # ç‰¹æ®Šå‚æ•°å¤„ç†
    if hasattr(cfg.train, 'deepspeed_config') and cfg.train.deepspeed_config:
        training_config["deepspeed"] = cfg.train.deepspeed_config
    
    if hasattr(cfg.train, 'gradient_clipping'):
        training_config["max_grad_norm"] = cfg.train.gradient_clipping
    
    # W&Bè®¾ç½®
    if cfg.train.report_to and "wandb" in cfg.train.report_to:
        training_config.update({
            "report_to": ["wandb"],
            "run_name": cfg.wandb.run_name
        })
    else:
        training_config["report_to"] = []
    
    if eval_converted_dataset:
        training_config["per_device_eval_batch_size"] = cfg.train.batch_size
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer_config = {
        "model": model,
        "tokenizer": tokenizer,
        "data_collator": UnslothVisionDataCollator(model, tokenizer),
        "train_dataset": converted_dataset,
        "args": SFTConfig(**training_config),
    }
    
    if eval_converted_dataset:
        trainer_config["eval_dataset"] = eval_converted_dataset
    
    trainer = SFTTrainer(**trainer_config)
    
    # å¼€å§‹è®­ç»ƒ
    resume_checkpoint = getattr(cfg.run, 'resume_from_checkpoint', None)
    trainer.train(resume_from_checkpoint=resume_checkpoint)
    
    print("âœ… è®­ç»ƒå®Œæˆ")
    
    # ======================================================================
    # ğŸ’¾ æ¨¡å‹ä¿å­˜
    # ======================================================================
    save_path = os.path.join(cfg.train.output_dir, "final_model")
    model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    # W&Bæ¨¡å‹ä¸Šä¼ 
    if cfg.wandb and getattr(cfg.wandb, 'log_model', False):
        try:
            wandb.save(os.path.join(save_path, "*"))
            print("ğŸ“¤ æ¨¡å‹å·²ä¸Šä¼ åˆ°W&B")
        except Exception as e:
            print(f"âš ï¸ W&Bæ¨¡å‹ä¸Šä¼ å¤±è´¥: {e}")
    
    if cfg.wandb and cfg.train.report_to and "wandb" in cfg.train.report_to:
        wandb.finish()


if __name__ == "__main__":
    main() 