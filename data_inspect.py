#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†æ£€æŸ¥å™¨ - Spatial VLM QA Dataset
ä¸‹è½½å¹¶åˆ†æ https://huggingface.co/datasets/Litian2002/spatialvlm_qa
"""

import os
import matplotlib.pyplot as plt
import numpy as np
from datasets import load_dataset
from PIL import Image
import pandas as pd

def download_and_inspect_dataset():
    """ä¸‹è½½å¹¶æ£€æŸ¥ spatialvlm_qa æ•°æ®é›†"""
    
    print("ğŸ”„ æ­£åœ¨ä¸‹è½½ Spatial VLM QA æ•°æ®é›†...")
    print("ğŸ“ æ•°æ®é›†åœ°å€: https://huggingface.co/datasets/Litian2002/spatialvlm_qa")
    
    try:
        # ä¸‹è½½æ•°æ®é›†
        dataset = load_dataset("Litian2002/spatialvlm_qa")
        print("âœ… æ•°æ®é›†ä¸‹è½½æˆåŠŸ!")
        
        # åŸºæœ¬ä¿¡æ¯
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®é›†åŸºæœ¬ä¿¡æ¯")
        print("="*60)
        print(f"æ•°æ®é›†åç§°: Spatial VLM QA")
        print(f"æ€»æ ·æœ¬æ•°: {len(dataset['train'])}")
        print(f"æ•°æ®åˆ†å‰²: {list(dataset.keys())}")
        
        # æŸ¥çœ‹å­—æ®µç»“æ„
        train_data = dataset['train']
        print(f"\nğŸ“‹ æ•°æ®å­—æ®µ:")
        for key, feature in train_data.features.items():
            print(f"  â€¢ {key}: {feature}")
        
        # éšæœºé‡‡æ ·å‡ ä¸ªä¾‹å­
        print("\n" + "="*60)
        print("ğŸ” éšæœºæ ·æœ¬æ£€æŸ¥ (å‰5ä¸ª)")
        print("="*60)
        
        for i in range(min(5, len(train_data))):
            sample = train_data[i]
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"  é—®é¢˜: {sample['question']}")
            print(f"  ç­”æ¡ˆ: {sample['answer']}")
            print(f"  å›¾åƒå°ºå¯¸: {sample['image'].size}")
        
        # ç»Ÿè®¡åˆ†æ
        print("\n" + "="*60)
        print("ğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡åˆ†æ")
        print("="*60)
        
        # é—®é¢˜é•¿åº¦åˆ†æ
        questions = [sample['question'] for sample in train_data.select(range(1000))]  # å–å‰1000ä¸ªæ ·æœ¬åˆ†æ
        answers = [sample['answer'] for sample in train_data.select(range(1000))]
        
        q_lengths = [len(q.split()) for q in questions]
        a_lengths = [len(a.split()) for a in answers]
        
        print(f"é—®é¢˜é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(q_lengths):.1f} è¯")
        print(f"  æœ€çŸ­: {min(q_lengths)} è¯")
        print(f"  æœ€é•¿: {max(q_lengths)} è¯")
        
        print(f"\nç­”æ¡ˆé•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(a_lengths):.1f} è¯")
        print(f"  æœ€çŸ­: {min(a_lengths)} è¯")
        print(f"  æœ€é•¿: {max(a_lengths)} è¯")
        
        # é—®é¢˜ç±»å‹åˆ†æ
        print(f"\né—®é¢˜ç±»å‹åˆ†æ (å‰1000ä¸ªæ ·æœ¬):")
        question_types = {}
        for q in questions:
            q_lower = q.lower()
            if any(word in q_lower for word in ['distance', 'how far', 'how distant']):
                question_types['è·ç¦»æµ‹é‡'] = question_types.get('è·ç¦»æµ‹é‡', 0) + 1
            elif any(word in q_lower for word in ['front', 'back', 'behind']):
                question_types['å‰åå…³ç³»'] = question_types.get('å‰åå…³ç³»', 0) + 1
            elif any(word in q_lower for word in ['width', 'height', 'measure']):
                question_types['å°ºå¯¸æµ‹é‡'] = question_types.get('å°ºå¯¸æµ‹é‡', 0) + 1
            elif any(word in q_lower for word in ['positioned', 'located']):
                question_types['ä½ç½®å…³ç³»'] = question_types.get('ä½ç½®å…³ç³»', 0) + 1
            else:
                question_types['å…¶ä»–'] = question_types.get('å…¶ä»–', 0) + 1
        
        for qtype, count in question_types.items():
            print(f"  â€¢ {qtype}: {count} ({count/len(questions)*100:.1f}%)")
        
        # ç­”æ¡ˆç±»å‹åˆ†æ  
        print(f"\nç­”æ¡ˆç±»å‹åˆ†æ:")
        numeric_answers = 0
        yes_no_answers = 0
        other_answers = 0
        
        for a in answers:
            a_lower = a.lower().strip()
            if a_lower in ['yes', 'no', 'yes.', 'no.']:
                yes_no_answers += 1
            elif any(char.isdigit() for char in a_lower):
                numeric_answers += 1
            else:
                other_answers += 1
        
        print(f"  â€¢ æ•°å€¼å‹ç­”æ¡ˆ: {numeric_answers} ({numeric_answers/len(answers)*100:.1f}%)")
        print(f"  â€¢ æ˜¯å¦å‹ç­”æ¡ˆ: {yes_no_answers} ({yes_no_answers/len(answers)*100:.1f}%)")
        print(f"  â€¢ å…¶ä»–ç­”æ¡ˆ: {other_answers} ({other_answers/len(answers)*100:.1f}%)")
        
        # ä¿å­˜å‡ ä¸ªç¤ºä¾‹å›¾åƒ
        print("\n" + "="*60)
        print("ğŸ’¾ ä¿å­˜ç¤ºä¾‹å›¾åƒ")
        print("="*60)
        
        os.makedirs("dataset_samples", exist_ok=True)
        
        for i in range(min(3, len(train_data))):
            sample = train_data[i]
            img = sample['image']
            img.save(f"dataset_samples/sample_{i+1}.png")
            
            # ä¿å­˜é—®ç­”å¯¹
            with open(f"dataset_samples/sample_{i+1}_qa.txt", "w", encoding="utf-8") as f:
                f.write(f"é—®é¢˜: {sample['question']}\n")
                f.write(f"ç­”æ¡ˆ: {sample['answer']}\n")
            
            print(f"  ä¿å­˜æ ·æœ¬ {i+1}: sample_{i+1}.png + sample_{i+1}_qa.txt")
        
        print(f"\nâœ… ç¤ºä¾‹æ–‡ä»¶ä¿å­˜åˆ°: ./dataset_samples/")
        
        # åˆ›å»ºç®€å•çš„å¯è§†åŒ–
        create_visualization(questions, answers, q_lengths, a_lengths)
        
        return dataset
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return None

def create_visualization(questions, answers, q_lengths, a_lengths):
    """åˆ›å»ºæ•°æ®é›†ç»Ÿè®¡å¯è§†åŒ–"""
    
    print("\nğŸ“Š åˆ›å»ºç»Ÿè®¡å›¾è¡¨...")
    
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']  # æ”¯æŒä¸­æ–‡
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
    
    # é—®é¢˜é•¿åº¦åˆ†å¸ƒ
    ax1.hist(q_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.set_title('Question Length Distribution')
    ax1.set_xlabel('Number of Words')
    ax1.set_ylabel('Frequency')
    ax1.grid(True, alpha=0.3)
    
    # ç­”æ¡ˆé•¿åº¦åˆ†å¸ƒ
    ax2.hist(a_lengths, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
    ax2.set_title('Answer Length Distribution')
    ax2.set_xlabel('Number of Words')
    ax2.set_ylabel('Frequency')
    ax2.grid(True, alpha=0.3)
    
    # é—®é¢˜å…³é”®è¯ç»Ÿè®¡
    keywords = ['distance', 'front', 'back', 'width', 'height', 'measure', 'far']
    keyword_counts = []
    for keyword in keywords:
        count = sum(1 for q in questions if keyword in q.lower())
        keyword_counts.append(count)
    
    ax3.bar(keywords, keyword_counts, color='lightgreen', alpha=0.7)
    ax3.set_title('Question Keywords Frequency')
    ax3.set_xlabel('Keywords')
    ax3.set_ylabel('Count')
    ax3.tick_params(axis='x', rotation=45)
    
    # ç­”æ¡ˆç±»å‹é¥¼å›¾
    yes_count = sum(1 for a in answers if a.lower().strip() in ['yes', 'yes.'])
    no_count = sum(1 for a in answers if a.lower().strip() in ['no', 'no.'])
    numeric_count = sum(1 for a in answers if any(c.isdigit() for c in a) and a.lower().strip() not in ['yes', 'no', 'yes.', 'no.'])
    other_count = len(answers) - yes_count - no_count - numeric_count
    
    labels = ['Yes', 'No', 'Numeric', 'Other']
    sizes = [yes_count, no_count, numeric_count, other_count]
    colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightyellow']
    
    ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    ax4.set_title('Answer Type Distribution')
    
    plt.tight_layout()
    plt.savefig('dataset_analysis.png', dpi=150, bbox_inches='tight')
    print("  ğŸ“Š ç»Ÿè®¡å›¾è¡¨ä¿å­˜ä¸º: dataset_analysis.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Spatial VLM QA æ•°æ®é›†æ£€æŸ¥å™¨")
    print("=" * 60)
    
    dataset = download_and_inspect_dataset()
    
    if dataset:
        print("\nğŸ‰ æ•°æ®é›†æ£€æŸ¥å®Œæˆ!")
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  â€¢ dataset_samples/ - ç¤ºä¾‹å›¾åƒå’Œé—®ç­”å¯¹")
        print("  â€¢ dataset_analysis.png - ç»Ÿè®¡åˆ†æå›¾è¡¨")
        
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("  1. è¿™æ˜¯ä¸€ä¸ªåˆæˆçš„3Dç©ºé—´æ¨ç†æ•°æ®é›†")
        print("  2. åŒ…å«40kä¸ªå›¾åƒ-é—®ç­”å¯¹")
        print("  3. ä¸»è¦ç”¨äºè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›")
        print("  4. é—®é¢˜ç±»å‹æ¶µç›–è·ç¦»æµ‹é‡ã€ä½ç½®å…³ç³»ã€å°ºå¯¸ç­‰")
        
    else:
        print("\nâŒ æ•°æ®é›†æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–åº“")

if __name__ == "__main__":
    main()
