# Hydraé…ç½®ç®¡ç†ç³»ç»Ÿ

æœ¬é¡¹ç›®ä½¿ç”¨Hydra/OmegaConfè¿›è¡Œé…ç½®ç®¡ç†ï¼Œæä¾›çµæ´»ä¸”å¯é‡ç°çš„å®éªŒè®¾ç½®ã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
VLM-Spatial/
â”œâ”€â”€ conf/                          # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ train_qwen.yaml           # é»˜è®¤è®­ç»ƒé…ç½®ï¼ˆå®Œæ•´ç‰ˆï¼‰
â”‚   â””â”€â”€ train_qwen_small.yaml     # å°è§„æ¨¡è®­ç»ƒé…ç½®
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ train.py                  # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ run_examples.sh              # ä½¿ç”¨ç¤ºä¾‹è„šæœ¬
â””â”€â”€ requirements_hydra.txt       # Hydraç›¸å…³ä¾èµ–
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install hydra-core omegaconf tqdm numpy
# æˆ–è€…
pip install -r requirements_hydra.txt
```

### 2. åŸºæœ¬ä½¿ç”¨

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
python scripts/train.py

# ä½¿ç”¨æŒ‡å®šé…ç½®æ–‡ä»¶
python scripts/train.py --config-name=train_qwen_small
```

### 3. è¿è¡Œæ—¶å‚æ•°è¦†ç›– (â­ æ¨è)

```bash
# è°ƒæ•´LoRAå‚æ•°
python scripts/train.py lora.r=96 lora.alpha=96

# è°ƒæ•´è®­ç»ƒå‚æ•°
python scripts/train.py train.batch_size=4 train.learning_rate=3e-4

# ç»„åˆå¤šä¸ªå‚æ•°
python scripts/train.py lora.r=48 train.batch_size=6 train.num_epochs=3
```

## ğŸ“‹ é…ç½®æ–‡ä»¶ç»“æ„

### ğŸƒ è¿è¡Œç¯å¢ƒé…ç½®
```yaml
run:
  cuda_visible_devices: "1"          # GPUè®¾å¤‡ID
  seed: 3407                         # å…¨å±€éšæœºç§å­
  resume_from_checkpoint: null       # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
```

### ğŸ¤– æ¨¡å‹é…ç½®
```yaml
model:
  name: unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit
  load_in_4bit: true
  torch_dtype: "auto"               # æ•°æ®ç±»å‹
  trust_remote_code: true           # å…è®¸è‡ªå®šä¹‰ä»£ç 
  max_seq_length: 4096              # åºåˆ—é•¿åº¦
  device_map: "auto"                # è®¾å¤‡æ˜ å°„
  rope_scaling: null                # RoPEç¼©æ”¾é…ç½®
```

### ğŸ¯ LoRAé…ç½®
```yaml
lora:
  r: 64                           # LoRAçŸ©é˜µçš„ç§©
  alpha: 64                       # ç¼©æ”¾å› å­
  dropout: 0.05                   # Dropoutç‡
  bias: "none"                    # åç½®è®¾ç½®
  use_rslora: false               # æ’åºç¨³å®šLoRA
  target_modules: "all-linear"    # ç›®æ ‡æ¨¡å—
  # ... å¾®è°ƒé€‰é¡¹
```

### ğŸ“Š æ•°æ®é…ç½®
```yaml
data:
  dataset_name: "unsloth/LaTeX_OCR"
  split: "train"
  max_samples: null               # æ ·æœ¬æ•°é™åˆ¶
  num_proc: 4                     # å¤„ç†è¿›ç¨‹æ•°
  instruction: "Write the LaTeX representation for this image."
  shuffle: true                   # æ•°æ®æ‰“ä¹±
  streaming: false                # æµå¼æ¨¡å¼
```

### ğŸ‹ï¸ è®­ç»ƒé…ç½®
```yaml
train:
  # åŸºç¡€å‚æ•°
  batch_size: 8
  grad_accum: 2
  num_epochs: 2
  learning_rate: 2.0e-4
  
  # ä¼˜åŒ–å™¨è®¾ç½®
  optim: "paged_adamw_8bit"
  lr_scheduler_type: "linear"
  warmup_steps: 50
  weight_decay: 0.01
  
  # ç²¾åº¦å’Œå†…å­˜
  fp16: false
  bf16: true
  gradient_checkpointing: false
  
  # æ—¥å¿—å’Œä¿å­˜
  logging_steps: 10
  save_strategy: "steps"
  save_steps: 100
  output_dir: "outputs"
```

### ğŸ“ˆ W&Bé…ç½®
```yaml
wandb:
  project: "latex_ocr"
  entity: null                    # å›¢é˜Ÿåç§°
  run_name: "qwen_lora64"
  tags: ["qwen", "vision", "lora64"]
  notes: null
  log_model: true                 # ä¸Šä¼ æ¨¡å‹
```

## ğŸ”§ ä¸»è¦ç‰¹æ€§å’Œæ”¹è¿›

### 1. **å®Œæ•´çš„å‚æ•°åˆ†ç»„**
- ğŸƒ è¿è¡Œç¯å¢ƒï¼šGPUã€ç§å­ã€æ£€æŸ¥ç‚¹æ¢å¤
- ğŸ¤– æ¨¡å‹é…ç½®ï¼šå®Œæ•´çš„æ¨¡å‹åŠ è½½å‚æ•°
- ğŸ¯ LoRAé…ç½®ï¼šé«˜çº§LoRAé€‰é¡¹
- ğŸ“Š æ•°æ®é…ç½®ï¼šæ•°æ®å¤„ç†å’ŒåŠ è½½
- ğŸ‹ï¸ è®­ç»ƒé…ç½®ï¼šå®Œæ•´çš„è®­ç»ƒå‚æ•°
- ğŸ“ˆ W&Bé…ç½®ï¼šå®éªŒè·Ÿè¸ªè®¾ç½®

### 2. **å¢å¼ºçš„åŠŸèƒ½**
- âœ… éšæœºç§å­è®¾ç½®ï¼Œç¡®ä¿å¯é‡ç°æ€§
- âœ… æ£€æŸ¥ç‚¹æ¢å¤æ”¯æŒ
- âœ… å¯è‡ªå®šä¹‰çš„æŒ‡ä»¤æç¤º
- âœ… æ•°æ®æ‰“ä¹±å’Œæµå¼åŠ è½½
- âœ… å®Œæ•´çš„W&Bé›†æˆ
- âœ… è¿›åº¦æ¡å’Œè¯¦ç»†æ—¥å¿—

### 3. **çµæ´»çš„é…ç½®ç®¡ç†**
- âœ… è¿è¡Œæ—¶å‚æ•°è¦†ç›–
- âœ… å¤šé…ç½®æ–‡ä»¶æ”¯æŒ
- âœ… å®Œæ•´çš„å‚æ•°éªŒè¯
- âœ… æ¸…æ™°çš„é…ç½®åˆ†ç»„

## ğŸ’¡ é«˜çº§ç”¨æ³•ç¤ºä¾‹

### å¿«é€Ÿå®éªŒ
```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆå°æ•°æ®é‡ï¼‰
python scripts/train.py data.max_samples=100 train.num_epochs=1

# è°ƒè¯•æ¨¡å¼ï¼ˆæå°é…ç½®ï¼‰
python scripts/train.py --config-name=train_qwen_small data.max_samples=10
```

### æ€§èƒ½ä¼˜åŒ–
```bash
# å°GPUé…ç½®
python scripts/train.py lora.r=16 train.batch_size=2 train.grad_accum=8

# å¤§GPUé…ç½®  
python scripts/train.py lora.r=128 train.batch_size=16 train.grad_accum=1

# å¯ç”¨DeepSpeed
python scripts/train.py train.deepspeed_config=./ds_config.json
```

### å®éªŒç®¡ç†
```bash
# ä¸åŒå­¦ä¹ ç‡å®éªŒ
python scripts/train.py wandb.run_name=exp_lr_high train.learning_rate=5e-4
python scripts/train.py wandb.run_name=exp_lr_low train.learning_rate=1e-4

# ä¸åŒLoRAé…ç½®å®éªŒ
python scripts/train.py wandb.run_name=lora32 lora.r=32 lora.alpha=32
python scripts/train.py wandb.run_name=lora64 lora.r=64 lora.alpha=64
```

### ä»æ£€æŸ¥ç‚¹æ¢å¤
```bash
# ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤
python scripts/train.py run.resume_from_checkpoint=./outputs/checkpoint-100

# æ›´æ”¹é…ç½®ç»§ç»­è®­ç»ƒ
python scripts/train.py run.resume_from_checkpoint=./outputs/checkpoint-100 train.learning_rate=1e-4
```

## ğŸ¯ é…ç½®éªŒè¯

è„šæœ¬ä¼šè‡ªåŠ¨éªŒè¯é…ç½®å‚æ•°ï¼Œå¹¶åœ¨å¯åŠ¨æ—¶æ˜¾ç¤ºï¼š
- ğŸ”§ è¿è¡Œç¯å¢ƒè®¾ç½®
- ğŸ² éšæœºç§å­
- ğŸ“ˆ W&Båˆå§‹åŒ–çŠ¶æ€
- ğŸš€ æ¨¡å‹åŠ è½½å‚æ•°
- ğŸ”§ LoRAé…ç½®
- ğŸ“Š æ•°æ®é›†ä¿¡æ¯
- ğŸ”„ æ•°æ®è½¬æ¢è¿›åº¦

## ğŸ“š æ›´å¤šä¿¡æ¯

- æŸ¥çœ‹ `run_examples.sh` äº†è§£ä½¿ç”¨ç¤ºä¾‹
- å‚è€ƒå…·ä½“é…ç½®æ–‡ä»¶ä¸­çš„æ³¨é‡Š
- æ‰€æœ‰é…ç½®éƒ½ä¼šè‡ªåŠ¨è®°å½•åˆ°W&Bä¸­ 