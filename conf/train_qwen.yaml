# conf/train_qwen.yaml - Unsloth Vision LoRA 微调主配置文件
# =============================================================================

# ┌─────────────────────────────────────────────────────────────────────────┐
# │                          🏃 运行环境配置                               │
# └─────────────────────────────────────────────────────────────────────────┘
run:
  cuda_visible_devices: "1"          # GPU设备ID
  seed: 3407                         # 全局随机种子，确保可重现性
  resume_from_checkpoint: null       # 从检查点恢复训练的路径

# ┌─────────────────────────────────────────────────────────────────────────┐
# │                          🤖 模型配置                                   │
# └─────────────────────────────────────────────────────────────────────────┘
model:
  name: unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit  # 模型名称或路径
  load_in_4bit: true              # 使用4bit NF4加载基础权重
  torch_dtype: "auto"             # 数据类型: "auto"|"bf16"|"fp16"|"fp32"
  # todo 下面5个未验证
  gradient_ckpt: "unsloth"        # 启用Unsloth的激活检查点
  trust_remote_code: true         # 允许自定义Qwen代码
  max_seq_length: 2048             # 序列长度硬限制
  device_map: "auto"              # 设备映射: "auto"|"sequential"|显式字典
  rope_scaling: null              # RoPE缩放配置，如 {"type":"linear","factor":2}

# ┌─────────────────────────────────────────────────────────────────────────┐
# │                          🎯 LoRA配置                                  │
# └─────────────────────────────────────────────────────────────────────────┘
lora:
  r: 64                           # LoRA矩阵的秩
  alpha: 64                       # 缩放因子，通常等于r
  dropout: 0.05                   # LoRA适配器内的dropout
  bias: "none"                    # 偏置设置: "none"|"lora_only"|"all"
  finetune_vision_layers: true    # 在视觉塔上应用LoRA
  finetune_language_layers: true  # 在语言层上应用LoRA
  finetune_attention_modules: true # 在Q,K,V,O线性层应用LoRA
  finetune_mlp_modules: true      # 在FFN线性层应用LoRA
  use_rslora: false               # 使用Rank Stabilized LoRA
  loftq_config: null              # LoftQ配置字典（可选）
  target_modules: "all-linear"    # 目标模块或正则表达式列表
  enable_gradient_checkpointing: false  # 额外的LoRA级检查点

# ┌─────────────────────────────────────────────────────────────────────────┐
# │                          📊 数据配置                                   │
# └─────────────────────────────────────────────────────────────────────────┘
data:
  dataset_name: "unsloth/LaTeX_OCR"   # HF数据集路径或本地文件
  split: "train"                      # 数据集分割名称
  max_samples: null                   # 最大样本数，null表示使用全部数据
  num_proc: 1                          # 数据处理进程数（固定单线程）
  instruction: "Write the LaTeX representation for this image."  # 指令提示
  shuffle: true                        # 是否打乱数据
  streaming: false                    # 启用HF流式模式
  val_split_ratio: 0.0                # 验证集比例，0表示无验证集

# ┌─────────────────────────────────────────────────────────────────────────┐
# │                          🏋️ 训练配置                                   │
# └─────────────────────────────────────────────────────────────────────────┘
train:
  # 基础训练参数
  batch_size: 512                       # 每GPU的mini-batch大小
  grad_accum: 1                       # 梯度累积步数
  num_epochs: 2                       # 训练轮数
  max_steps: null                     # 最大步数，覆盖num_epochs
  
  # 优化器和学习率
  learning_rate: 2.0e-4               # 学习率
  lr_scheduler_type: "linear"         # 学习率调度器: linear|cosine|constant
  warmup_steps: 5                    # 学习率预热步数
  weight_decay: 0.01                  # 权重衰减
  optim: "paged_adamw_8bit"           # 优化器类型
  gradient_clipping: 1.0              # 梯度裁剪
  
  # 精度和内存
  max_seq_length: 2048                # 训练序列长度
  fp16: false                         # FP16混合精度训练
  bf16: true                          # BF16混合精度训练
  gradient_checkpointing: false       # HF级梯度检查点
  
  # 日志和保存
  logging_steps: 1                   # 每N步记录日志
  save_strategy: "steps"              # 保存策略: "steps"|"epoch"
  save_steps: 20                     # 保存频率
  output_dir: "outputs"               # 检查点保存目录
  
  # 评估配置
  evaluation_strategy: "no"           # 评估策略: "steps"|"epoch"|"no"
  eval_steps: 500                     # 评估频率
  
  # 系统配置
  dataloader_num_workers: 32           # 数据加载器工作进程数
  deepspeed_config: null              # DeepSpeed ZeRO-3配置文件路径
  report_to: ["wandb"]                # 实验跟踪: "none"|"wandb"|"tensorboard"
  disable_tqdm: false                 # 禁用进度条

# ┌─────────────────────────────────────────────────────────────────────────┐
# │                          📈 W&B配置                                    │
# └─────────────────────────────────────────────────────────────────────────┘
wandb:
  project: "latex_ocr"                               # W&B项目名称
  entity: null                        # W&B团队名称
  run_name: "qwen2.5-vl-3b-instruct-lora"            # 实验运行名称
  tags: ["qwen", "vision", "lora"]  # 实验标签
  notes: null                         # 实验备注
  # log_model: true                     # 上传最佳检查点作为artifact
