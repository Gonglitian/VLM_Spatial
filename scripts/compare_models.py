#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹å¯¹æ¯”è„šæœ¬ - æ¯”è¾ƒå¾®è°ƒå‰åçš„Qwen2.5-VLæ¨¡å‹è¾“å‡º
å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œfinal_modelçš„æ¨ç†ç»“æœï¼Œæ™ºèƒ½è¯„ä¼°ç­”æ¡ˆè´¨é‡
"""

import os
import torch
import random
import argparse
import re
from datasets import load_dataset
from unsloth import FastVisionModel
from PIL import Image
import pandas as pd
from tqdm import tqdm


def extract_numbers(text):
    """ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ŒåŒ…æ‹¬å°æ•°"""
    numbers = re.findall(r'-?\d+\.?\d*', text)
    return [float(n) for n in numbers if n and n != '.']


def is_numerical_question(ground_truth):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼å‹é—®é¢˜"""
    numbers = extract_numbers(ground_truth)
    return len(numbers) > 0


def calculate_numerical_similarity(ground_truth, response):
    """ä¸“é—¨è®¡ç®—æ•°å€¼å‹ç­”æ¡ˆçš„ç›¸ä¼¼åº¦"""
    gt_numbers = extract_numbers(ground_truth)
    resp_numbers = extract_numbers(response)
    
    if not gt_numbers:
        return 0.0, "éæ•°å€¼å‹ç­”æ¡ˆ"
    
    if not resp_numbers:
        return 0.0, "å›ç­”ä¸­æ— æ•°å€¼"
    
    # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ•°å€¼å¯¹
    min_diff = float('inf')
    best_gt = None
    best_resp = None
    
    for gt_num in gt_numbers:
        for resp_num in resp_numbers:
            diff = abs(gt_num - resp_num)
            if diff < min_diff:
                min_diff = diff
                best_gt = gt_num
                best_resp = resp_num
    
    # è®¡ç®—ç›¸å¯¹è¯¯å·®å’Œç»å¯¹è¯¯å·®
    if best_gt != 0:
        relative_error = min_diff / abs(best_gt)
    else:
        relative_error = min_diff
    
    # ç²¾ç»†åŒ–çš„æ•°å€¼ç›¸ä¼¼åº¦è¯„åˆ†
    if min_diff == 0:
        return 1.0, f"æ•°å€¼å®Œå…¨åŒ¹é… ({best_resp})"
    elif min_diff <= 0.01:
        return 0.95, f"æ•°å€¼æåº¦æ¥è¿‘ (å·®å€¼: {min_diff:.3f})"
    elif min_diff <= 0.05:
        return 0.90, f"æ•°å€¼éå¸¸æ¥è¿‘ (å·®å€¼: {min_diff:.3f})"
    elif min_diff <= 0.1:
        return 0.85, f"æ•°å€¼å¾ˆæ¥è¿‘ (å·®å€¼: {min_diff:.3f})"
    elif min_diff <= 0.2:
        return 0.75, f"æ•°å€¼æ¥è¿‘ (å·®å€¼: {min_diff:.3f})"
    elif min_diff <= 0.5:
        return 0.60, f"æ•°å€¼è¾ƒæ¥è¿‘ (å·®å€¼: {min_diff:.3f})"
    elif min_diff <= 1.0:
        return 0.40, f"æ•°å€¼æœ‰å·®è· (å·®å€¼: {min_diff:.3f})"
    elif min_diff <= 2.0:
        return 0.25, f"æ•°å€¼å·®è·è¾ƒå¤§ (å·®å€¼: {min_diff:.3f})"
    elif relative_error <= 0.5:  # ç›¸å¯¹è¯¯å·®50%ä»¥å†…
        return 0.15, f"æ•°å€¼å·®è·å¤§ (å·®å€¼: {min_diff:.3f}, ç›¸å¯¹è¯¯å·®: {relative_error:.2%})"
    else:
        return 0.05, f"æ•°å€¼å·®è·å¾ˆå¤§ (å·®å€¼: {min_diff:.3f}, ç›¸å¯¹è¯¯å·®: {relative_error:.2%})"


def calculate_answer_similarity(ground_truth, response):
    """è®¡ç®—ç­”æ¡ˆä¸çœŸå®å€¼çš„ç›¸ä¼¼åº¦"""
    gt_lower = ground_truth.lower().strip()
    resp_lower = response.lower().strip()
    
    # 1. å®Œå…¨åŒ¹é…
    if gt_lower == resp_lower:
        return 1.0, "å®Œå…¨åŒ¹é…"
    
    # 2. åŒ…å«åŒ¹é…
    if gt_lower in resp_lower or resp_lower in gt_lower:
        return 0.8, "åŒ…å«åŒ¹é…"
    
    # 3. ä¼˜å…ˆå¤„ç†æ•°å€¼å‹é—®é¢˜
    if is_numerical_question(ground_truth):
        numerical_sim, numerical_reason = calculate_numerical_similarity(ground_truth, response)
        # å¯¹äºæ•°å€¼å‹é—®é¢˜ï¼Œå¦‚æœæ‰¾åˆ°äº†æ•°å€¼ï¼Œä¼˜å…ˆä½¿ç”¨æ•°å€¼ç›¸ä¼¼åº¦
        if "æ— æ•°å€¼" not in numerical_reason and "éæ•°å€¼" not in numerical_reason:
            return numerical_sim, f"[æ•°å€¼å‹] {numerical_reason}"
        # å¦‚æœæ²¡æ‰¾åˆ°æ•°å€¼ï¼Œé™çº§å¤„ç†ä½†ä¿æŒæ ‡è¯†
        else:
            # ç»§ç»­ä¸‹é¢çš„æ–‡æœ¬åŒ¹é…ï¼Œä½†æ ‡è®°ä¸ºæ•°å€¼å‹é—®é¢˜
            pass
    
    # 4. ä¼ ç»Ÿæ•°å€¼åŒ¹é…ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
    gt_numbers = extract_numbers(ground_truth)
    resp_numbers = extract_numbers(response)
    
    if gt_numbers and resp_numbers:
        # æ‰¾æœ€æ¥è¿‘çš„æ•°å€¼
        min_diff = float('inf')
        for gt_num in gt_numbers:
            for resp_num in resp_numbers:
                diff = abs(gt_num - resp_num)
                min_diff = min(min_diff, diff)
        
        # åŸºç¡€æ•°å€¼ç›¸ä¼¼åº¦ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ä½œä¸ºå…¼å®¹ï¼‰
        if min_diff == 0:
            return 0.9, f"æ•°å€¼å®Œå…¨åŒ¹é…"
        elif min_diff <= 0.5:
            return 0.7, f"æ•°å€¼æ¥è¿‘ (å·®å€¼: {min_diff:.2f})"
        elif min_diff <= 1.0:
            return 0.5, f"æ•°å€¼è¾ƒæ¥è¿‘ (å·®å€¼: {min_diff:.2f})"
        else:
            return 0.2, f"æ•°å€¼ç›¸å·®è¾ƒå¤§ (å·®å€¼: {min_diff:.2f})"
    
    # 5. æ˜¯å¦å‹ç­”æ¡ˆåŒ¹é…
    yes_words = ['yes', 'true', 'æ˜¯', 'å¯¹', 'æ­£ç¡®']
    no_words = ['no', 'false', 'å¦', 'ä¸', 'é”™è¯¯']
    
    gt_is_yes = any(word in gt_lower for word in yes_words)
    gt_is_no = any(word in gt_lower for word in no_words)
    resp_is_yes = any(word in resp_lower for word in yes_words)
    resp_is_no = any(word in resp_lower for word in no_words)
    
    if (gt_is_yes and resp_is_yes) or (gt_is_no and resp_is_no):
        return 0.6, "æ˜¯å¦å‹åŒ¹é…"
    elif (gt_is_yes and resp_is_no) or (gt_is_no and resp_is_yes):
        return 0.1, "æ˜¯å¦å‹ç›¸å"
    
    # 6. è¯æ±‡é‡å åº¦
    gt_words = set(gt_lower.split())
    resp_words = set(resp_lower.split())
    
    if gt_words and resp_words:
        overlap = len(gt_words.intersection(resp_words))
        union = len(gt_words.union(resp_words))
        jaccard = overlap / union if union > 0 else 0
        
        # å¦‚æœæ˜¯æ•°å€¼å‹é—®é¢˜ä½†æ²¡åŒ¹é…åˆ°æ•°å€¼ï¼Œé™ä½è¯æ±‡é‡å åº¦çš„æƒé‡
        if is_numerical_question(ground_truth):
            jaccard_penalty = 0.5  # æ•°å€¼å‹é—®é¢˜çš„è¯æ±‡åŒ¹é…æƒé‡å‡åŠ
            if jaccard > 0.5:
                return 0.2, f"[æ•°å€¼å‹-è¯æ±‡] è¯æ±‡é‡å åº¦é«˜ä½†ç¼ºä¹æ•°å€¼ ({jaccard:.2f})"
            elif jaccard > 0.2:
                return 0.15, f"[æ•°å€¼å‹-è¯æ±‡] è¯æ±‡é‡å åº¦ä¸­ç­‰ä½†ç¼ºä¹æ•°å€¼ ({jaccard:.2f})"
            else:
                return 0.05, f"[æ•°å€¼å‹-è¯æ±‡] è¯æ±‡é‡å åº¦ä½ä¸”ç¼ºä¹æ•°å€¼ ({jaccard:.2f})"
        else:
            # éæ•°å€¼å‹é—®é¢˜çš„æ­£å¸¸è¯æ±‡åŒ¹é…
            if jaccard > 0.5:
                return 0.4, f"è¯æ±‡é‡å åº¦é«˜ ({jaccard:.2f})"
            elif jaccard > 0.2:
                return 0.3, f"è¯æ±‡é‡å åº¦ä¸­ç­‰ ({jaccard:.2f})"
            else:
                return 0.1, f"è¯æ±‡é‡å åº¦ä½ ({jaccard:.2f})"
    
    return 0.0, "æ— åŒ¹é…"


def save_sample_image(image, sample_index, question, ground_truth, original_response, finetuned_response, better_model, output_dir="comparison_samples"):
    """ä¿å­˜æ ·æœ¬å›¾ç‰‡å’Œè¯¦ç»†ä¿¡æ¯"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜å›¾ç‰‡
    image_path = os.path.join(output_dir, f"sample_{sample_index}.png")
    image.save(image_path)
    
    # ä¿å­˜è¯¦ç»†ä¿¡æ¯
    info_path = os.path.join(output_dir, f"sample_{sample_index}_info.txt")
    with open(info_path, "w", encoding="utf-8") as f:
        f.write(f"æ ·æœ¬ç´¢å¼•: {sample_index}\n")
        f.write(f"é—®é¢˜: {question}\n")
        f.write(f"æ ‡å‡†ç­”æ¡ˆ: {ground_truth}\n")
        f.write(f"åŸå§‹æ¨¡å‹å›ç­”: {original_response}\n")
        f.write(f"å¾®è°ƒæ¨¡å‹å›ç­”: {finetuned_response}\n")
        f.write(f"æ›´å¥½çš„æ¨¡å‹: {better_model}\n")
    
    return image_path, info_path


def load_models():
    """åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒåçš„æ¨¡å‹"""
    print("ğŸš€ åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    print("  ğŸ“¥ åŠ è½½åŸå§‹æ¨¡å‹: unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit")
    original_model, original_tokenizer = FastVisionModel.from_pretrained(
        model_name="unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit",
        load_in_4bit=True
    )
    print("  âœ… åŸå§‹æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
    finetuned_model_path = "outputs/final_model"
    if not os.path.exists(finetuned_model_path):
        # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
        possible_paths = [
            "final_model",
            "outputs/checkpoint-20/final_model", 
            "outputs/checkpoint-32/final_model",
            "outputs/2025-06-03/14-58-59/outputs/final_model"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                finetuned_model_path = path
                break
        else:
            raise FileNotFoundError("âŒ æ‰¾ä¸åˆ°å¾®è°ƒåçš„æ¨¡å‹ï¼è¯·æ£€æŸ¥è·¯å¾„")
    
    print(f"  ğŸ“¥ åŠ è½½å¾®è°ƒæ¨¡å‹: {finetuned_model_path}")
    finetuned_model, finetuned_tokenizer = FastVisionModel.from_pretrained(
        model_name=finetuned_model_path,
        load_in_4bit=True
    )
    print("  âœ… å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼
    FastVisionModel.for_inference(original_model)
    FastVisionModel.for_inference(finetuned_model)
    
    return (original_model, original_tokenizer), (finetuned_model, finetuned_tokenizer)


def load_test_samples(num_samples=10):
    """ä»æ•°æ®é›†ä¸­éšæœºåŠ è½½æµ‹è¯•æ ·æœ¬"""
    print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®é›†: Litian2002/spatialvlm_qa")
    
    try:
        dataset = load_dataset("Litian2002/spatialvlm_qa", split="train")
        print(f"  ğŸ“ˆ æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        total_samples = len(dataset)
        random_indices = random.sample(range(total_samples), min(num_samples, total_samples))
        
        test_samples = []
        for idx in random_indices:
            sample = dataset[idx]
            test_samples.append({
                "index": idx,
                "image": sample["image"],
                "question": sample["question"],
                "ground_truth": sample["answer"]
            })
        
        print(f"  âœ… éšæœºé€‰æ‹©äº† {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return test_samples
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return []


def generate_response(model, tokenizer, image, question):
    """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”"""
    try:
        # å‚è€ƒunslothå®˜æ–¹ç¤ºä¾‹çš„æ­£ç¡®æ ¼å¼
        messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},  # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦ä¼ imageå‚æ•°
                    {"type": "text", "text": question}
                ]
            }
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        input_text = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # æ­£ç¡®çš„è°ƒç”¨æ–¹å¼ï¼šimageä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
        inputs = tokenizer(
            image,           # å›¾åƒä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
            input_text,      # æ–‡æœ¬ä½œä¸ºç¬¬äºŒä¸ªå‚æ•°
            add_special_tokens=False,
            return_tensors="pt",
        ).to(model.device)
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.1,
                do_sample=True,
                use_cache=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # æå–å›ç­”éƒ¨åˆ†
        input_token_length = inputs['input_ids'].shape[1]
        response_tokens = outputs[0][input_token_length:]
        response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()
        
        return response if response else "ğŸ¤– æ¨¡å‹æœªäº§ç”Ÿæœ‰æ•ˆå›ç­”"
        
    except Exception as e:
        print(f"âš ï¸ æ¨ç†é”™è¯¯è¯¦æƒ…: {e}")
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä»…æ–‡æœ¬æ¨ç†
        try:
            simple_input = tokenizer(
                f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n",
                return_tensors="pt"
            ).to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **simple_input,
                    max_new_tokens=128,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            input_length = simple_input['input_ids'].shape[1]
            response_tokens = outputs[0][input_length:]
            response = tokenizer.decode(response_tokens, skip_special_tokens=True).strip()
            
            return f"ğŸ“ [ä»…æ–‡æœ¬æ¨¡å¼] {response}" if response else "âŒ å¤‡ç”¨æ¨ç†ä¹Ÿå¤±è´¥"
            
        except Exception as backup_error:
            return f"âŒ å®Œå…¨å¤±è´¥: {str(e)} | å¤‡ç”¨: {str(backup_error)}"


def compare_models_on_samples(original_model_info, finetuned_model_info, test_samples):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹åœ¨æµ‹è¯•æ ·æœ¬ä¸Šçš„è¡¨ç°"""
    original_model, original_tokenizer = original_model_info
    finetuned_model, finetuned_tokenizer = finetuned_model_info
    
    print("\nğŸ” å¼€å§‹æ¨¡å‹å¯¹æ¯”æµ‹è¯•...")
    
    results = []
    
    for i, sample in enumerate(tqdm(test_samples, desc="ğŸ§ª æµ‹è¯•æ ·æœ¬")):
        print(f"\n{'='*80}")
        print(f"ğŸ“ æµ‹è¯•æ ·æœ¬ {i+1}/{len(test_samples)} (æ•°æ®é›†ç´¢å¼•: {sample['index']})")
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºé—®é¢˜å’Œå›¾åƒä¿¡æ¯
        print(f"â“ é—®é¢˜: {sample['question']}")
        print(f"ğŸ–¼ï¸  å›¾åƒå°ºå¯¸: {sample['image'].size}")
        print(f"âœ… æ ‡å‡†ç­”æ¡ˆ: {sample['ground_truth']}")
        
        # åŸå§‹æ¨¡å‹æ¨ç†
        print("\nğŸ¤– åŸå§‹æ¨¡å‹æ¨ç†ä¸­...")
        original_response = generate_response(
            original_model, original_tokenizer, 
            sample['image'], sample['question']
        )
        print(f"ğŸ“¤ åŸå§‹æ¨¡å‹å›ç­”: {original_response}")
        
        # å¾®è°ƒæ¨¡å‹æ¨ç†  
        print("\nğŸ¯ å¾®è°ƒæ¨¡å‹æ¨ç†ä¸­...")
        finetuned_response = generate_response(
            finetuned_model, finetuned_tokenizer,
            sample['image'], sample['question']
        )
        print(f"ğŸ“¤ å¾®è°ƒæ¨¡å‹å›ç­”: {finetuned_response}")
        
        # æ™ºèƒ½è¯„ä¼°ç­”æ¡ˆè´¨é‡
        original_similarity, original_reason = calculate_answer_similarity(
            sample['ground_truth'], original_response
        )
        finetuned_similarity, finetuned_reason = calculate_answer_similarity(
            sample['ground_truth'], finetuned_response
        )
        
        # åˆ¤æ–­å“ªä¸ªæ¨¡å‹æ›´å¥½
        if finetuned_similarity > original_similarity:
            better_model = "å¾®è°ƒæ¨¡å‹"
            winner_emoji = "ğŸ¯"
        elif original_similarity > finetuned_similarity:
            better_model = "åŸå§‹æ¨¡å‹"
            winner_emoji = "ğŸ¤–"
        else:
            better_model = "å¹³å±€"
            winner_emoji = "ğŸ¤"
        
        print(f"\nğŸ“Š æ™ºèƒ½è¯„ä¼°:")
        print(f"  ğŸ¤– åŸå§‹æ¨¡å‹ç›¸ä¼¼åº¦: {original_similarity:.2f} ({original_reason})")
        print(f"  ğŸ¯ å¾®è°ƒæ¨¡å‹ç›¸ä¼¼åº¦: {finetuned_similarity:.2f} ({finetuned_reason})")
        print(f"  {winner_emoji} æ›´å¥½çš„æ¨¡å‹: {better_model}")
        
        # ä¿å­˜æ ·æœ¬å›¾ç‰‡å’Œä¿¡æ¯
        image_path, info_path = save_sample_image(
            sample['image'], sample['index'], sample['question'],
            sample['ground_truth'], original_response, finetuned_response, better_model
        )
        print(f"  ğŸ’¾ æ ·æœ¬å·²ä¿å­˜: {image_path}")
        
        # ä¿å­˜ç»“æœ
        results.append({
            "sample_index": sample['index'],
            "question": sample['question'],
            "ground_truth": sample['ground_truth'],
            "original_response": original_response,
            "finetuned_response": finetuned_response,
            "original_similarity": original_similarity,
            "finetuned_similarity": finetuned_similarity,
            "original_reason": original_reason,
            "finetuned_reason": finetuned_reason,
            "better_model": better_model,
            "image_size": str(sample['image'].size),
            "image_path": image_path,
            "info_path": info_path
        })
    
    return results


def save_results(results, output_file="model_comparison_results.csv"):
    """ä¿å­˜å¯¹æ¯”ç»“æœåˆ°CSVæ–‡ä»¶"""
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file}")
    
    df = pd.DataFrame(results)
    df.to_csv(output_file, index=False, encoding='utf-8')
    
    print("âœ… ç»“æœä¿å­˜å®Œæˆ!")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ è¯¦ç»†å¯¹æ¯”ç»Ÿè®¡:")
    print(f"  æ€»æµ‹è¯•æ ·æœ¬: {len(results)}")
    
    # åˆ†ç±»ç»Ÿè®¡ï¼šæ•°å€¼å‹ vs éæ•°å€¼å‹é—®é¢˜
    numerical_results = []
    non_numerical_results = []
    
    for result in results:
        if is_numerical_question(result['ground_truth']):
            numerical_results.append(result)
        else:
            non_numerical_results.append(result)
    
    print(f"  ğŸ“Š æ•°å€¼å‹é—®é¢˜: {len(numerical_results)} ({len(numerical_results)/len(results)*100:.1f}%)")
    print(f"  ğŸ“ éæ•°å€¼å‹é—®é¢˜: {len(non_numerical_results)} ({len(non_numerical_results)/len(results)*100:.1f}%)")
    
    # ç»Ÿè®¡è·èƒœæƒ…å†µ
    original_wins = sum(1 for r in results if r['better_model'] == 'åŸå§‹æ¨¡å‹')
    finetuned_wins = sum(1 for r in results if r['better_model'] == 'å¾®è°ƒæ¨¡å‹')
    ties = sum(1 for r in results if r['better_model'] == 'å¹³å±€')
    
    print(f"\nğŸ† æ€»ä½“æ¨¡å‹å¯¹æ¯”ç»“æœ:")
    print(f"  ğŸ¤– åŸå§‹æ¨¡å‹è·èƒœ: {original_wins}/{len(results)} ({original_wins/len(results)*100:.1f}%)")
    print(f"  ğŸ¯ å¾®è°ƒæ¨¡å‹è·èƒœ: {finetuned_wins}/{len(results)} ({finetuned_wins/len(results)*100:.1f}%)")
    print(f"  ğŸ¤ å¹³å±€: {ties}/{len(results)} ({ties/len(results)*100:.1f}%)")
    
    # æ•°å€¼å‹é—®é¢˜çš„ä¸“é—¨ç»Ÿè®¡
    if numerical_results:
        num_original_wins = sum(1 for r in numerical_results if r['better_model'] == 'åŸå§‹æ¨¡å‹')
        num_finetuned_wins = sum(1 for r in numerical_results if r['better_model'] == 'å¾®è°ƒæ¨¡å‹')
        num_ties = sum(1 for r in numerical_results if r['better_model'] == 'å¹³å±€')
        
        print(f"\nğŸ”¢ æ•°å€¼å‹é—®é¢˜å¯¹æ¯”ç»“æœ:")
        print(f"  ğŸ¤– åŸå§‹æ¨¡å‹è·èƒœ: {num_original_wins}/{len(numerical_results)} ({num_original_wins/len(numerical_results)*100:.1f}%)")
        print(f"  ğŸ¯ å¾®è°ƒæ¨¡å‹è·èƒœ: {num_finetuned_wins}/{len(numerical_results)} ({num_finetuned_wins/len(numerical_results)*100:.1f}%)")
        print(f"  ğŸ¤ å¹³å±€: {num_ties}/{len(numerical_results)} ({num_ties/len(numerical_results)*100:.1f}%)")
        
        # æ•°å€¼å‹é—®é¢˜çš„å¹³å‡ç›¸ä¼¼åº¦
        num_avg_original = sum(r['original_similarity'] for r in numerical_results) / len(numerical_results)
        num_avg_finetuned = sum(r['finetuned_similarity'] for r in numerical_results) / len(numerical_results)
        
        print(f"\nğŸ“Š æ•°å€¼å‹é—®é¢˜å¹³å‡ç›¸ä¼¼åº¦:")
        print(f"  ğŸ¤– åŸå§‹æ¨¡å‹: {num_avg_original:.3f}")
        print(f"  ğŸ¯ å¾®è°ƒæ¨¡å‹: {num_avg_finetuned:.3f}")
        print(f"  ğŸ“ˆ æ”¹è¿›å¹…åº¦: {(num_avg_finetuned - num_avg_original):.3f}")
        
        # æ•°å€¼ç²¾åº¦ç»Ÿè®¡
        high_precision_finetuned = sum(1 for r in numerical_results if r['finetuned_similarity'] >= 0.8)
        high_precision_original = sum(1 for r in numerical_results if r['original_similarity'] >= 0.8)
        
        print(f"\nğŸ¯ æ•°å€¼é«˜ç²¾åº¦åŒ¹é… (ç›¸ä¼¼åº¦â‰¥0.8):")
        print(f"  ğŸ¤– åŸå§‹æ¨¡å‹: {high_precision_original}/{len(numerical_results)} ({high_precision_original/len(numerical_results)*100:.1f}%)")
        print(f"  ğŸ¯ å¾®è°ƒæ¨¡å‹: {high_precision_finetuned}/{len(numerical_results)} ({high_precision_finetuned/len(numerical_results)*100:.1f}%)")
    
    # éæ•°å€¼å‹é—®é¢˜çš„ç»Ÿè®¡
    if non_numerical_results:
        nnum_original_wins = sum(1 for r in non_numerical_results if r['better_model'] == 'åŸå§‹æ¨¡å‹')
        nnum_finetuned_wins = sum(1 for r in non_numerical_results if r['better_model'] == 'å¾®è°ƒæ¨¡å‹')
        nnum_ties = sum(1 for r in non_numerical_results if r['better_model'] == 'å¹³å±€')
        
        print(f"\nğŸ“ éæ•°å€¼å‹é—®é¢˜å¯¹æ¯”ç»“æœ:")
        print(f"  ğŸ¤– åŸå§‹æ¨¡å‹è·èƒœ: {nnum_original_wins}/{len(non_numerical_results)} ({nnum_original_wins/len(non_numerical_results)*100:.1f}%)")
        print(f"  ğŸ¯ å¾®è°ƒæ¨¡å‹è·èƒœ: {nnum_finetuned_wins}/{len(non_numerical_results)} ({nnum_finetuned_wins/len(non_numerical_results)*100:.1f}%)")
        print(f"  ğŸ¤ å¹³å±€: {nnum_ties}/{len(non_numerical_results)} ({nnum_ties/len(non_numerical_results)*100:.1f}%)")
    
    # æ€»ä½“å¹³å‡ç›¸ä¼¼åº¦
    avg_original_similarity = sum(r['original_similarity'] for r in results) / len(results)
    avg_finetuned_similarity = sum(r['finetuned_similarity'] for r in results) / len(results)
    
    print(f"\nğŸ“Š æ€»ä½“å¹³å‡ç›¸ä¼¼åº¦:")
    print(f"  ğŸ¤– åŸå§‹æ¨¡å‹: {avg_original_similarity:.3f}")
    print(f"  ğŸ¯ å¾®è°ƒæ¨¡å‹: {avg_finetuned_similarity:.3f}")
    print(f"  ğŸ“ˆ æ”¹è¿›å¹…åº¦: {(avg_finetuned_similarity - avg_original_similarity):.3f}")
    
    # æœ€ä½³å’Œæœ€å·®æ ·æœ¬
    best_improvement = max(results, key=lambda x: x['finetuned_similarity'] - x['original_similarity'])
    worst_regression = min(results, key=lambda x: x['finetuned_similarity'] - x['original_similarity'])
    
    print(f"\nğŸ¯ å…³é”®æ ·æœ¬:")
    print(f"  ğŸš€ æœ€å¤§æ”¹è¿›æ ·æœ¬: #{best_improvement['sample_index']} (æ”¹è¿›: {best_improvement['finetuned_similarity'] - best_improvement['original_similarity']:.3f})")
    print(f"  âš ï¸  æœ€å¤§é€€æ­¥æ ·æœ¬: #{worst_regression['sample_index']} (é€€æ­¥: {worst_regression['finetuned_similarity'] - worst_regression['original_similarity']:.3f})")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¯¹æ¯”å¾®è°ƒå‰åçš„Qwen2.5-VLæ¨¡å‹")
    parser.add_argument("--num_samples", type=int, default=100, help="æµ‹è¯•æ ·æœ¬æ•°é‡")
    parser.add_argument("--output", type=str, default="model_comparison_results.csv", help="è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    print("ğŸ” Qwen2.5-VL æ¨¡å‹å¯¹æ¯”å·¥å…· (å¢å¼ºæ•°å€¼å‹é—®é¢˜è¯„ä¼°)")
    print("="*80)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        original_model_info, finetuned_model_info = load_models()
        
        # 2. åŠ è½½æµ‹è¯•æ ·æœ¬
        test_samples = load_test_samples(args.num_samples)
        if not test_samples:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ ·æœ¬")
            return
        
        # 3. æ‰§è¡Œå¯¹æ¯”æµ‹è¯•
        results = compare_models_on_samples(
            original_model_info, finetuned_model_info, test_samples
        )
        
        # 4. ä¿å­˜ç»“æœ
        save_results(results, args.output)
        
        print("\nğŸ‰ æ¨¡å‹å¯¹æ¯”å®Œæˆ!")
        print(f"ğŸ“„ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: {args.output}")
        print(f"ğŸ“ æµ‹è¯•æ ·æœ¬ä¿å­˜åœ¨: comparison_samples/")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 