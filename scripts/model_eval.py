#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•æ¨¡å‹è¯„ä¼°è„šæœ¬ - vLLMæ‰¹é‡æ¨ç†ç‰ˆæœ¬
ä½¿ç”¨Hydraé…ç½®ç®¡ç†å‚æ•°ï¼Œæ”¯æŒæ–‡æœ¬ä¸€è‡´æ€§å’Œæ•°å€¼è¯¯å·®ç™¾åˆ†æ¯”åŒé‡è¯„ä¼°æŒ‡æ ‡
ä¸“æ³¨äºvLLMæ‰¹é‡æ¨ç†ï¼Œå¤§å¹…æå‡è¯„ä¼°é€Ÿåº¦
å›¾è¡¨å†…å®¹ä½¿ç”¨è‹±æ–‡æ˜¾ç¤º
"""

import os
import torch
import random
import hydra
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from omegaconf import DictConfig, OmegaConf
from datasets import load_dataset
from PIL import Image
from tqdm import tqdm
from difflib import SequenceMatcher
import warnings
warnings.filterwarnings('ignore')

# vLLMç›¸å…³å¯¼å…¥
try:
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
    print("âœ… vLLMåº“å¯ç”¨ï¼Œå¯ç”¨æ‰¹é‡æ¨ç†åŠ é€Ÿ")
except ImportError:
    VLLM_AVAILABLE = False
    print("âŒ vLLMåº“ä¸å¯ç”¨ï¼Œè¯·å®‰è£…vLLM: pip install vllm")
    exit(1)

# è®¾ç½®è‹±æ–‡å­—ä½“å’Œç»˜å›¾æ ·å¼
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def extract_numbers(text):
    """ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ŒåŒ…æ‹¬å°æ•°å’Œè´Ÿæ•°"""
    if pd.isna(text) or text is None:
        return []
    numbers = re.findall(r'-?\d+\.?\d*', str(text))
    return [float(n) for n in numbers if n and n != '.']


def is_numerical_question(ground_truth):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼å‹é—®é¢˜"""
    numbers = extract_numbers(ground_truth)
    return len(numbers) > 0


def calculate_text_consistency(ground_truth, response, config):
    """
    æŒ‡æ ‡1: æ–‡æœ¬ä¸€è‡´æ€§è®¡ç®—
    ä½¿ç”¨å¤šç§æ–¹æ³•è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
    """
    if not ground_truth or not response:
        return 0.0, "ç©ºæ–‡æœ¬"
    
    gt_lower = str(ground_truth).lower().strip()
    resp_lower = str(response).lower().strip()
    
    # 1. å®Œå…¨åŒ¹é…
    if gt_lower == resp_lower:
        return 1.0, "å®Œå…¨åŒ¹é…"
    
    # 2. åŒ…å«åŒ¹é…
    if gt_lower in resp_lower or resp_lower in gt_lower:
        return 0.8, "åŒ…å«åŒ¹é…"
    
    # 3. åºåˆ—åŒ¹é…åº¦ (åŸºäºdifflib)
    sequence_similarity = SequenceMatcher(None, gt_lower, resp_lower).ratio()
    
    # 4. è¯æ±‡é‡å åº¦ (Jaccardç›¸ä¼¼åº¦)
    gt_words = set(gt_lower.split())
    resp_words = set(resp_lower.split())
    
    if gt_words and resp_words:
        intersection = len(gt_words.intersection(resp_words))
        union = len(gt_words.union(resp_words))
        jaccard_similarity = intersection / union if union > 0 else 0
    else:
        jaccard_similarity = 0
    
    # 5. æ˜¯å¦å‹ç­”æ¡ˆåŒ¹é…
    yes_words = ['yes', 'true', 'æ˜¯', 'å¯¹', 'æ­£ç¡®', 'correct']
    no_words = ['no', 'false', 'å¦', 'ä¸', 'é”™è¯¯', 'incorrect', 'wrong']
    
    gt_is_yes = any(word in gt_lower for word in yes_words)
    gt_is_no = any(word in gt_lower for word in no_words)
    resp_is_yes = any(word in resp_lower for word in yes_words)
    resp_is_no = any(word in resp_lower for word in no_words)
    
    boolean_match = 0
    boolean_reason = ""
    if (gt_is_yes and resp_is_yes) or (gt_is_no and resp_is_no):
        boolean_match = 0.6
        boolean_reason = "æ˜¯å¦å‹åŒ¹é…"
    elif (gt_is_yes and resp_is_no) or (gt_is_no and resp_is_yes):
        boolean_match = 0.1
        boolean_reason = "æ˜¯å¦å‹ç›¸å"
    
    # ä½¿ç”¨é…ç½®ä¸­çš„æƒé‡è¿›è¡Œç»¼åˆè¯„åˆ†
    weights = config.metrics.text_consistency
    weighted_score = (sequence_similarity * weights.sequence_weight + 
                     jaccard_similarity * weights.jaccard_weight + 
                     boolean_match * weights.boolean_weight)
    
    max_similarity = max(sequence_similarity, jaccard_similarity, boolean_match, weighted_score)
    
    if max_similarity >= 0.8:
        reason = f"é«˜æ–‡æœ¬ä¸€è‡´æ€§ (åºåˆ—:{sequence_similarity:.2f}, è¯æ±‡:{jaccard_similarity:.2f})"
    elif max_similarity >= 0.5:
        reason = f"ä¸­ç­‰æ–‡æœ¬ä¸€è‡´æ€§ (åºåˆ—:{sequence_similarity:.2f}, è¯æ±‡:{jaccard_similarity:.2f})"
    elif boolean_reason:
        reason = boolean_reason
    else:
        reason = f"ä½æ–‡æœ¬ä¸€è‡´æ€§ (åºåˆ—:{sequence_similarity:.2f}, è¯æ±‡:{jaccard_similarity:.2f})"
    
    return max_similarity, reason


def calculate_numerical_error_percentage(ground_truth, response):
    """
    æŒ‡æ ‡2: æ•°å€¼è¯¯å·®ç™¾åˆ†æ¯”è®¡ç®—
    ä¸“é—¨é’ˆå¯¹æ•°å€¼å‹é—®é¢˜è®¡ç®—è¯¯å·®ç™¾åˆ†æ¯”
    """
    gt_numbers = extract_numbers(ground_truth)
    resp_numbers = extract_numbers(response)
    
    if not gt_numbers:
        return None, "éæ•°å€¼å‹é—®é¢˜"
    
    if not resp_numbers:
        return None, "å›ç­”ä¸­æ— æ•°å€¼"
    
    # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ•°å€¼å¯¹
    min_error_percentage = float('inf')
    best_gt = None
    best_resp = None
    
    for gt_num in gt_numbers:
        for resp_num in resp_numbers:
            if gt_num != 0:
                error_percentage = abs(gt_num - resp_num) / abs(gt_num) * 100
            else:
                error_percentage = abs(resp_num) * 100  # å½“çœŸå®å€¼ä¸º0æ—¶
            
            if error_percentage < min_error_percentage:
                min_error_percentage = error_percentage
                best_gt = gt_num
                best_resp = resp_num
    
    return min_error_percentage, f"æ•°å€¼å¯¹æ¯”: {best_resp} vs {best_gt}"


def calculate_comprehensive_similarity(ground_truth, response, config):
    """
    ç»¼åˆç›¸ä¼¼åº¦è®¡ç®— - ç»“åˆä¸¤ä¸ªæŒ‡æ ‡
    """
    # æŒ‡æ ‡1: æ–‡æœ¬ä¸€è‡´æ€§
    text_consistency, text_reason = calculate_text_consistency(ground_truth, response, config)
    
    # æŒ‡æ ‡2: æ•°å€¼è¯¯å·®ç™¾åˆ†æ¯”ï¼ˆä»…å¯¹æ•°å€¼å‹é—®é¢˜ï¼‰
    if is_numerical_question(ground_truth):
        error_percentage, numerical_reason = calculate_numerical_error_percentage(ground_truth, response)
        
        if error_percentage is not None:
            # ä½¿ç”¨é…ç½®ä¸­çš„è¯„åˆ†ç­–ç•¥å°†è¯¯å·®ç™¾åˆ†æ¯”è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
            scoring = config.metrics.numerical_scoring
            
            if error_percentage == 0:
                numerical_score = scoring.perfect_match
            elif error_percentage <= 1:
                numerical_score = scoring.excellent
            elif error_percentage <= 5:
                numerical_score = scoring.very_good
            elif error_percentage <= 10:
                numerical_score = scoring.good
            elif error_percentage <= 20:
                numerical_score = scoring.acceptable
            elif error_percentage <= 50:
                numerical_score = scoring.poor
            else:
                numerical_score = scoring.very_poor
            
            # ä½¿ç”¨é…ç½®ä¸­çš„æƒé‡è¿›è¡Œç»¼åˆè¯„åˆ†
            weights = config.metrics.score_weights
            final_score = (weights.numerical_precision * numerical_score + 
                          weights.text_consistency * text_consistency)
            reason = f"[æ•°å€¼å‹] è¯¯å·®:{error_percentage:.1f}%, æ–‡æœ¬ä¸€è‡´æ€§:{text_consistency:.2f}"
        else:
            # æ•°å€¼å‹é—®é¢˜ä½†æœªæ‰¾åˆ°æ•°å€¼ï¼Œä½¿ç”¨é™æƒç³»æ•°
            penalty = config.metrics.score_weights.no_numerical_penalty
            final_score = text_consistency * penalty
            reason = f"[æ•°å€¼å‹-æ— æ•°å€¼] {text_reason}"
    else:
        # éæ•°å€¼å‹é—®é¢˜ï¼Œä¸»è¦ä½¿ç”¨æ–‡æœ¬ä¸€è‡´æ€§
        final_score = text_consistency
        reason = f"[æ–‡æœ¬å‹] {text_reason}"
    
    return final_score, reason


def load_model(config):
    """åŠ è½½æ¨¡å‹ - æ”¯æŒvLLMæ‰¹é‡æ¨ç†"""
    global VLLM_AVAILABLE  # å£°æ˜ä¸ºå…¨å±€å˜é‡
    print("ğŸš€ åŠ è½½æ¨¡å‹...")
    
    model_id = config.model.model_id
    
    if VLLM_AVAILABLE:
        # ä½¿ç”¨vLLMåŠ è½½HuggingFaceæ¨¡å‹ï¼ˆä¸æ”¯æŒæœ¬åœ°LoRAï¼‰
        print(f"  ğŸš€ ä½¿ç”¨vLLMæ‰¹é‡æ¨ç†å¼•æ“åŠ è½½: {model_id}")
        try:
            llm = LLM(
                model=model_id,
                trust_remote_code=True,
                max_model_len=4096,  # æ ¹æ®éœ€è¦è°ƒæ•´
            )
            
            print("  âœ… vLLMæ¨¡å‹åŠ è½½å®Œæˆ")
            return llm, None, f"vLLM({model_id})"
            
        except Exception as e:
            print(f"âŒ vLLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    else:
        # error
        raise ValueError(f"æ¨¡å‹ {model_id} ä¸å­˜åœ¨")
    
def load_test_samples(config):
    """ç›´æ¥åŠ è½½æ•°æ®é›†å¹¶éšæœºé‡‡æ ·æµ‹è¯•æ ·æœ¬"""
    num_samples = config.data.num_samples
    seed = config.run.seed
    dataset_name = config.data.dataset_name
    dataset_split = config.data.dataset_split
    
    print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®é›†: {dataset_name}")
    print(f"  ğŸ¯ ç›®æ ‡æ ·æœ¬æ•°é‡: {num_samples}")
    
    try:
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        random.seed(seed)
        
        # ç›´æ¥åŠ è½½æ•´ä¸ªæ•°æ®é›†
        print("  ğŸ“¦ åŠ è½½å®Œæ•´æ•°æ®é›†...")
        dataset = load_dataset(dataset_name, split=dataset_split)
        print(f"  ğŸ“ˆ æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        total_samples = len(dataset)
        if num_samples > total_samples:
            print(f"  âš ï¸ è¯·æ±‚æ ·æœ¬æ•°({num_samples})å¤§äºæ•°æ®é›†å¤§å°({total_samples})ï¼Œä½¿ç”¨å…¨éƒ¨æ ·æœ¬")
            num_samples = total_samples
        
        random_indices = random.sample(range(total_samples), num_samples)
        random_indices.sort()  # æ’åºä»¥ä¾¿æŸ¥çœ‹
        
        print(f"  ğŸ² éšæœºé€‰ä¸­ç´¢å¼•: {random_indices}")
        
        test_samples = []
        for idx in random_indices:
            sample = dataset[idx]
            test_samples.append({
                "index": idx,
                "image": sample["image"],
                "question": sample["question"],
                "ground_truth": sample["answer"]
            })
        
        print(f"  âœ… æˆåŠŸåŠ è½½ {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return test_samples
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return []


def batch_generate_responses_vllm(llm, test_samples, config):
    """ä½¿ç”¨vLLMæ‰¹é‡ç”Ÿæˆå›ç­”"""
    print("ğŸš€ ä½¿ç”¨vLLMæ‰¹é‡æ¨ç†...")
    
    # å‡†å¤‡æ‰¹é‡è¾“å…¥
    batch_inputs = []
    for sample in test_samples:
        # å‚è€ƒtrain.pyçš„æ•°æ®æ ¼å¼
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": sample["question"]},
                    {"type": "image", "image": sample["image"]}
                ]
            }
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        prompt = llm.get_tokenizer().apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        batch_inputs.append({
            "prompt": prompt,
            "multi_modal_data": {"image": sample["image"]}
        })
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    gen_config = config.generation
    sampling_params = SamplingParams(
        temperature=gen_config.temperature,
        max_tokens=gen_config.max_new_tokens,
        top_p=0.9,  # æ·»åŠ top_på‚æ•°
        stop=None,
    )
    
    print(f"  ğŸ“ æ‰¹é‡æ¨ç† {len(batch_inputs)} ä¸ªæ ·æœ¬...")
    
    try:
        # æ‰§è¡Œæ‰¹é‡æ¨ç†
        outputs = llm.generate(batch_inputs, sampling_params=sampling_params)
        
        # æå–å›ç­”
        responses = []
        for output in outputs:
            if output.outputs and len(output.outputs) > 0:
                response = output.outputs[0].text.strip()
                responses.append(response if response else "ğŸ¤– æ¨¡å‹æœªäº§ç”Ÿæœ‰æ•ˆå›ç­”")
            else:
                responses.append("âŒ æ¨ç†å¤±è´¥: æ— è¾“å‡º")
        
        print(f"  âœ… æ‰¹é‡æ¨ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(responses)} ä¸ªå›ç­”")
        return responses
        
    except Exception as e:
        print(f"âŒ vLLMæ‰¹é‡æ¨ç†å¤±è´¥: {e}")
        raise

def save_sample_image(image, sample_index, question, ground_truth, response, score, config):
    """ä¿å­˜æ ·æœ¬å›¾ç‰‡å’Œè¯¦ç»†ä¿¡æ¯"""
    base_dir = config.output.base_output_dir
    samples_dir = os.path.join(base_dir, config.output.samples_dir)
    os.makedirs(samples_dir, exist_ok=True)
    
    # ä¿å­˜å›¾ç‰‡
    image_path = os.path.join(samples_dir, f"sample_{sample_index}.png")
    image.save(image_path)
    
    # ä¿å­˜è¯¦ç»†ä¿¡æ¯
    info_path = os.path.join(samples_dir, f"sample_{sample_index}_info.txt")
    with open(info_path, "w", encoding="utf-8") as f:
        f.write(f"æ ·æœ¬ç´¢å¼•: {sample_index}\n")
        f.write(f"é—®é¢˜: {question}\n")
        f.write(f"æ ‡å‡†ç­”æ¡ˆ: {ground_truth}\n")
        f.write(f"æ¨¡å‹å›ç­”: {response}\n")
        f.write(f"è¯„ä¼°åˆ†æ•°: {score:.3f}\n")
    
    return image_path, info_path


def evaluate_model_on_samples(llm, model_path, test_samples, config):
    """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•æ ·æœ¬ä¸Šçš„è¡¨ç° - vLLMæ‰¹é‡æ¨ç†"""
    print(f"\nğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°æµ‹è¯•...")
    print(f"ğŸ“‹ è¯„ä¼°æ¨¡å‹: {model_path}")
    
    # ä½¿ç”¨vLLMæ‰¹é‡æ¨ç†
    print("ğŸš€ ä½¿ç”¨vLLMæ‰¹é‡æ¨ç†æ¨¡å¼")
    responses = batch_generate_responses_vllm(llm, test_samples, config)
    
    # å¤„ç†ç»“æœ
    results = []
    print(f"\nğŸ“Š å¤„ç†è¯„ä¼°ç»“æœ...")
    
    for i, (sample, response) in enumerate(zip(test_samples, responses)):
        print(f"\n{'='*80}")
        print(f"ğŸ“ å¤„ç†æ ·æœ¬ {i+1}/{len(test_samples)} (æ•°æ®é›†ç´¢å¼•: {sample['index']})")
        print(f"{'='*80}")
        
        # æ˜¾ç¤ºé—®é¢˜å’Œå›¾åƒä¿¡æ¯
        print(f"â“ é—®é¢˜: {sample['question']}")
        print(f"ğŸ–¼ï¸  å›¾åƒå°ºå¯¸: {sample['image'].size}")
        print(f"âœ… æ ‡å‡†ç­”æ¡ˆ: {sample['ground_truth']}")
        print(f"ğŸ“¤ æ¨¡å‹å›ç­”: {response}")
        
        # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
        similarity, reason = calculate_comprehensive_similarity(
            sample['ground_truth'], response, config
        )
        
        # è®¡ç®—æ•°å€¼è¯¯å·®ç™¾åˆ†æ¯”ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        error_pct = None
        if is_numerical_question(sample['ground_truth']):
            error, _ = calculate_numerical_error_percentage(sample['ground_truth'], response)
            error_pct = error
        
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"  ğŸ“ˆ ç›¸ä¼¼åº¦åˆ†æ•°: {similarity:.3f} ({reason})")
        if error_pct is not None:
            print(f"  ğŸ“Š æ•°å€¼è¯¯å·®: {error_pct:.1f}%")
        
        # ä¿å­˜æ ·æœ¬å›¾ç‰‡å’Œä¿¡æ¯
        image_path, info_path = save_sample_image(
            sample['image'], sample['index'], sample['question'],
            sample['ground_truth'], response, similarity, config
        )
        print(f"  ğŸ’¾ æ ·æœ¬å·²ä¿å­˜: {image_path}")
        
        # ä¿å­˜ç»“æœ
        results.append({
            "sample_index": sample['index'],
            "question": sample['question'],
            "ground_truth": sample['ground_truth'],
            "response": response,
            "similarity": similarity,
            "reason": reason,
            "error_percentage": error_pct,
            "is_numerical": is_numerical_question(sample['ground_truth']),
            "image_size": str(sample['image'].size),
            "image_path": image_path,
            "info_path": info_path
        })
    
    return results


def save_results_to_csv(results, config):
    """ä¿å­˜è¯„ä¼°ç»“æœåˆ°CSVæ–‡ä»¶"""
    base_dir = config.output.base_output_dir
    os.makedirs(base_dir, exist_ok=True)
    
    output_file = os.path.join(base_dir, config.output.csv_filename)
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file}")
    
    df = pd.DataFrame(results)
    df.to_csv(output_file, index=False, encoding='utf-8')
    
    print("âœ… ç»“æœä¿å­˜å®Œæˆ!")
    return df


def create_visualization(df, config):
    """åˆ›å»ºè‹±æ–‡å¯è§†åŒ–å›¾è¡¨"""
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    base_dir = config.output.base_output_dir
    output_dir = os.path.join(base_dir, config.output.visualization_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    # åº”ç”¨é…ç½®ä¸­çš„å¯è§†åŒ–è®¾ç½®
    viz_config = config.visualization
    plt.style.use(viz_config.style)
    plt.rcParams['figure.figsize'] = viz_config.figure_size
    plt.rcParams['font.size'] = viz_config.font_size
    
    colors = viz_config.colors
    
    # 1. åˆ›å»ºä¸»è¦è¯„ä¼°å›¾è¡¨
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1.1 ç›¸ä¼¼åº¦åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾
    ax1.hist(df['similarity'], bins=15, color=colors.primary, alpha=0.7, edgecolor='black')
    ax1.axvline(df['similarity'].mean(), color=colors.error, linestyle='--', linewidth=2, 
                label=f'Mean: {df["similarity"].mean():.3f}')
    ax1.set_xlabel('Similarity Score')
    ax1.set_ylabel('Number of Samples')
    ax1.set_title('Similarity Score Distribution', fontweight='bold', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 1.2 é—®é¢˜ç±»å‹åˆ†å¸ƒé¥¼å›¾
    type_counts = df['is_numerical'].value_counts()
    
    # æ ¹æ®å®é™…æ•°æ®åŠ¨æ€ç”Ÿæˆæ ‡ç­¾
    type_labels = []
    for is_num in type_counts.index:
        if is_num:
            type_labels.append('Numerical Questions')
        else:
            type_labels.append('Non-numerical Questions')
    
    # ç¡®ä¿é¢œè‰²æ•°é‡è¶³å¤Ÿ
    available_colors = [colors.secondary, colors.accent][:len(type_counts)]
    
    wedges, texts, autotexts = ax2.pie(type_counts.values, labels=type_labels, 
                                      autopct='%1.1f%%', colors=available_colors, startangle=90)
    ax2.set_title('Question Type Distribution', fontweight='bold', fontsize=14)
    
    # 1.3 åˆ†ç±»å‹ç›¸ä¼¼åº¦å¯¹æ¯”
    numerical_df = df[df['is_numerical']]
    non_numerical_df = df[~df['is_numerical']]
    
    categories = ['Numerical Questions', 'Non-numerical Questions']
    avg_scores = [
        numerical_df['similarity'].mean() if len(numerical_df) > 0 else 0,
        non_numerical_df['similarity'].mean() if len(non_numerical_df) > 0 else 0
    ]
    
    bars = ax3.bar(categories, avg_scores, color=[colors.accent, colors.secondary], alpha=0.8)
    ax3.set_ylabel('Average Similarity Score')
    ax3.set_title('Average Similarity by Question Type', fontweight='bold', fontsize=14)
    ax3.set_ylim(0, 1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 1.4 ç›¸ä¼¼åº¦ç­‰çº§åˆ†å¸ƒ
    def get_similarity_level(score):
        if score >= 0.9:
            return 'Excellent (â‰¥0.9)'
        elif score >= 0.7:
            return 'Good (0.7-0.9)'
        elif score >= 0.5:
            return 'Fair (0.5-0.7)'
        else:
            return 'Poor (<0.5)'
    
    df['similarity_level'] = df['similarity'].apply(get_similarity_level)
    level_counts = df['similarity_level'].value_counts()
    
    # ç¡®ä¿é¡ºåºæ­£ç¡®
    level_order = ['Excellent (â‰¥0.9)', 'Good (0.7-0.9)', 'Fair (0.5-0.7)', 'Poor (<0.5)']
    level_counts = level_counts.reindex(level_order, fill_value=0)
    
    bars = ax4.bar(range(len(level_counts)), level_counts.values, 
                   color=colors.primary, alpha=0.8)
    ax4.set_ylabel('Number of Samples')
    ax4.set_xlabel('Similarity Level')
    ax4.set_title('Similarity Level Distribution', fontweight='bold', fontsize=14)
    ax4.set_xticks(range(len(level_counts)))
    ax4.set_xticklabels(level_counts.index, rotation=45, ha='right')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{int(height)}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    output_path1 = os.path.join(output_dir, 'model_evaluation_overview.png')
    plt.savefig(output_path1, dpi=viz_config.dpi, bbox_inches='tight', 
                facecolor=colors.background)
    print(f"  ğŸ’¾ ä¿å­˜: {output_path1}")
    plt.close()
    
    # 2. æ•°å€¼å‹é—®é¢˜ä¸“é—¨åˆ†æ
    if len(numerical_df) > 0:
        valid_numerical = numerical_df.dropna(subset=['error_percentage'])
        
        if len(valid_numerical) > 0:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            
            # 2.1 æ•°å€¼è¯¯å·®åˆ†å¸ƒ
            ax1.hist(valid_numerical['error_percentage'], bins=10, color=colors.accent, 
                    alpha=0.7, edgecolor='black')
            ax1.axvline(valid_numerical['error_percentage'].mean(), color=colors.error, 
                       linestyle='--', linewidth=2, 
                       label=f'Mean Error: {valid_numerical["error_percentage"].mean():.1f}%')
            ax1.set_xlabel('Error Percentage (%)')
            ax1.set_ylabel('Number of Samples')
            ax1.set_title('Numerical Error Distribution', fontweight='bold', fontsize=14)
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # 2.2 æ•°å€¼ç²¾åº¦ç­‰çº§åˆ†å¸ƒ
            def get_error_level(error):
                if pd.isna(error):
                    return 'Cannot Calculate'
                elif error == 0:
                    return 'Perfect (0%)'
                elif error <= 5:
                    return 'Excellent (â‰¤5%)'
                elif error <= 20:
                    return 'Good (â‰¤20%)'
                else:
                    return 'Fair (>20%)'
            
            valid_numerical['error_level'] = valid_numerical['error_percentage'].apply(get_error_level)
            error_level_counts = valid_numerical['error_level'].value_counts()
            
            # ç¡®ä¿é¡ºåºæ­£ç¡®
            error_level_order = ['Perfect (0%)', 'Excellent (â‰¤5%)', 'Good (â‰¤20%)', 'Fair (>20%)']
            error_level_counts = error_level_counts.reindex(error_level_order, fill_value=0)
            
            # ä¸ºé¥¼å›¾å‡†å¤‡é¢œè‰²
            pie_colors = [colors.secondary, colors.accent, colors.primary, colors.error]
            available_pie_colors = pie_colors[:len(error_level_counts[error_level_counts > 0])]
            
            # åªæ˜¾ç¤ºæœ‰æ•°æ®çš„ç­‰çº§
            non_zero_counts = error_level_counts[error_level_counts > 0]
            
            wedges, texts, autotexts = ax2.pie(non_zero_counts.values, labels=non_zero_counts.index,
                                              autopct='%1.1f%%', colors=available_pie_colors, startangle=90)
            ax2.set_title(f'Numerical Precision Level Distribution\n({len(valid_numerical)} Numerical Questions)', 
                         fontweight='bold', fontsize=14)
            
            # 2.3 æ•°å€¼å‹é—®é¢˜ç›¸ä¼¼åº¦ vs è¯¯å·®æ•£ç‚¹å›¾
            ax3.scatter(valid_numerical['error_percentage'], valid_numerical['similarity'], 
                       color=colors.primary, alpha=0.6, s=50)
            ax3.set_xlabel('Error Percentage (%)')
            ax3.set_ylabel('Similarity Score')
            ax3.set_title('Similarity vs Numerical Error Relationship', fontweight='bold', fontsize=14)
            ax3.grid(True, alpha=0.3)
            
            # æ·»åŠ è¶‹åŠ¿çº¿
            if len(valid_numerical) > 1:
                z = np.polyfit(valid_numerical['error_percentage'], valid_numerical['similarity'], 1)
                p = np.poly1d(z)
                ax3.plot(valid_numerical['error_percentage'], p(valid_numerical['error_percentage']), 
                        color=colors.error, linestyle='--', alpha=0.8, label='Trend Line')
                ax3.legend()
            
            # 2.4 ä½åˆ†æ ·æœ¬åˆ†æï¼ˆç›¸ä¼¼åº¦<0.5çš„æ ·æœ¬ï¼‰
            low_score_samples = df[df['similarity'] < 0.5]
            if len(low_score_samples) > 0:
                type_counts = low_score_samples['is_numerical'].value_counts()
                
                # æ ¹æ®å®é™…æ•°æ®åŠ¨æ€ç”Ÿæˆæ ‡ç­¾
                type_labels = []
                for is_num in type_counts.index:
                    if is_num:
                        type_labels.append('Numerical')
                    else:
                        type_labels.append('Non-numerical')
                
                # ç¡®ä¿é¢œè‰²æ•°é‡è¶³å¤Ÿ
                low_score_colors = [colors.secondary, colors.accent][:len(type_counts)]
                
                wedges, texts, autotexts = ax4.pie(type_counts.values, labels=type_labels,
                                                  autopct='%1.1f%%', colors=low_score_colors, startangle=90)
                ax4.set_title(f'Low Score Sample Type Distribution\n(Similarity<0.5, {len(low_score_samples)} samples)', 
                             fontweight='bold', fontsize=14)
            else:
                ax4.text(0.5, 0.5, 'No Low Score Samples\n(All samples â‰¥0.5)', 
                        ha='center', va='center', transform=ax4.transAxes, 
                        fontsize=16, fontweight='bold', color=colors.primary)
                ax4.set_title('Low Score Sample Analysis', fontweight='bold', fontsize=14)
            
            plt.tight_layout()
            output_path2 = os.path.join(output_dir, 'numerical_questions_detailed_analysis.png')
            plt.savefig(output_path2, dpi=viz_config.dpi, bbox_inches='tight', 
                       facecolor=colors.background)
            print(f"  ğŸ’¾ ä¿å­˜: {output_path2}")
            plt.close()
    
    # 3. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    if config.output.create_detailed_report:
        create_evaluation_report(df, config, output_dir)
    
    print(f"\nğŸ‰ å¯è§†åŒ–å®Œæˆï¼å›¾è¡¨ä¿å­˜åœ¨: {output_dir}/")


def create_evaluation_report(df, config, output_dir):
    """åˆ›å»ºè¯¦ç»†è¯„ä¼°æŠ¥å‘Š"""
    print("ğŸ“‹ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    
    # åŸºæœ¬ç»Ÿè®¡
    total_samples = len(df)
    numerical_count = len(df[df['is_numerical']])
    non_numerical_count = total_samples - numerical_count
    
    # å¹³å‡åˆ†æ•°
    avg_similarity = df['similarity'].mean()
    
    # æ•°å€¼å‹é—®é¢˜ç»Ÿè®¡
    numerical_df = df[df['is_numerical']]
    numerical_stats = ""
    if len(numerical_df) > 0:
        valid_num_df = numerical_df.dropna(subset=['error_percentage'])
        if len(valid_num_df) > 0:
            avg_error = valid_num_df['error_percentage'].mean()
            min_error = valid_num_df['error_percentage'].min()
            max_error = valid_num_df['error_percentage'].max()
            
            # ç²¾åº¦ç­‰çº§ç»Ÿè®¡
            perfect_count = len(valid_num_df[valid_num_df['error_percentage'] == 0])
            excellent_count = len(valid_num_df[valid_num_df['error_percentage'] <= 5])
            good_count = len(valid_num_df[valid_num_df['error_percentage'] <= 20])
            
            numerical_stats = f"""
ğŸ“Š æ•°å€¼å‹é—®é¢˜è¯¦ç»†åˆ†æ:
  â€¢ æ•°å€¼å‹é—®é¢˜æ•°é‡: {len(numerical_df)} ({len(numerical_df)/total_samples*100:.1f}%)
  â€¢ æœ‰æ•ˆæ•°å€¼å¯¹æ¯”: {len(valid_num_df)} ä¸ªæ ·æœ¬
  â€¢ å¹³å‡æ•°å€¼è¯¯å·®: {avg_error:.2f}%
  â€¢ æœ€å°è¯¯å·®: {min_error:.2f}%
  â€¢ æœ€å¤§è¯¯å·®: {max_error:.2f}%
  
  ç²¾åº¦ç­‰çº§åˆ†å¸ƒ:
  â€¢ å®Œç¾åŒ¹é… (0%è¯¯å·®): {perfect_count} ä¸ª ({perfect_count/len(valid_num_df)*100:.1f}%)
  â€¢ ä¼˜ç§€ç²¾åº¦ (â‰¤5%è¯¯å·®): {excellent_count} ä¸ª ({excellent_count/len(valid_num_df)*100:.1f}%)
  â€¢ è‰¯å¥½ç²¾åº¦ (â‰¤20%è¯¯å·®): {good_count} ä¸ª ({good_count/len(valid_num_df)*100:.1f}%)
"""
    
    # ç›¸ä¼¼åº¦ç­‰çº§ç»Ÿè®¡
    excellent_sim = len(df[df['similarity'] >= 0.9])
    good_sim = len(df[df['similarity'] >= 0.7])
    medium_sim = len(df[df['similarity'] >= 0.5])
    poor_sim = len(df[df['similarity'] < 0.5])
    
    # æ¨¡å‹ä¿¡æ¯
    model_info = f"æ¨¡å‹ID: {config.model.model_id}"
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    report = f"""
ğŸ” å•æ¨¡å‹è¯„ä¼°ç»“æœæŠ¥å‘Š
==================================================

ğŸ“‹ è¯„ä¼°ä¿¡æ¯:
  â€¢ {model_info}
  â€¢ è¯„ä¼°æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
  â€¢ æ€»æµ‹è¯•æ ·æœ¬: {total_samples}
  â€¢ éšæœºç§å­: {config.run.seed}

ğŸ“Š åŸºæœ¬ç»Ÿè®¡:
  â€¢ æ•°å€¼å‹é—®é¢˜: {numerical_count} ({numerical_count/total_samples*100:.1f}%)
  â€¢ éæ•°å€¼å‹é—®é¢˜: {non_numerical_count} ({non_numerical_count/total_samples*100:.1f}%)
  â€¢ å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}

ğŸ“ˆ ç›¸ä¼¼åº¦ç­‰çº§åˆ†å¸ƒ:
  â€¢ ä¼˜ç§€ (â‰¥0.9): {excellent_sim} ä¸ª ({excellent_sim/total_samples*100:.1f}%)
  â€¢ è‰¯å¥½ (â‰¥0.7): {good_sim} ä¸ª ({good_sim/total_samples*100:.1f}%)
  â€¢ ä¸­ç­‰ (â‰¥0.5): {medium_sim} ä¸ª ({medium_sim/total_samples*100:.1f}%)
  â€¢ è¾ƒå·® (<0.5): {poor_sim} ä¸ª ({poor_sim/total_samples*100:.1f}%)

ğŸ“‹ è¯„ä¼°æŒ‡æ ‡è¯´æ˜:
  1. æ–‡æœ¬ä¸€è‡´æ€§è®¡ç®—: åŸºäºåºåˆ—åŒ¹é…ã€è¯æ±‡é‡å åº¦ç­‰å¤šç§æ–¹æ³•
  2. æ•°å€¼è¯¯å·®ç™¾åˆ†æ¯”: ä¸“é—¨é’ˆå¯¹æ•°å€¼å‹é—®é¢˜çš„ç²¾ç¡®åº¦è¯„ä¼°
  3. ç»¼åˆè¯„åˆ†: æ ¹æ®é—®é¢˜ç±»å‹åŠ¨æ€è°ƒæ•´æƒé‡

{numerical_stats}

ğŸ¯ æ€»ç»“:
  â€¢ æ¨¡å‹å¹³å‡è¡¨ç°: {"ä¼˜ç§€" if avg_similarity >= 0.8 else "è‰¯å¥½" if avg_similarity >= 0.6 else "ä¸­ç­‰" if avg_similarity >= 0.4 else "è¾ƒå·®"}
  â€¢ æ•´ä½“è¯„ä¼°åˆ†æ•°: {avg_similarity:.3f}/1.000
  â€¢ é«˜è´¨é‡å›ç­”æ¯”ä¾‹: {good_sim/total_samples*100:.1f}% (ç›¸ä¼¼åº¦â‰¥0.7)

==================================================
é…ç½®æ–‡ä»¶: eval_config.yaml
æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    # ä¿å­˜æŠ¥å‘Šåˆ°reportså­ç›®å½•
    base_dir = config.output.base_output_dir
    reports_dir = os.path.join(base_dir, config.output.reports_dir)
    os.makedirs(reports_dir, exist_ok=True)
    
    report_path = os.path.join(reports_dir, 'è¯„ä¼°æŠ¥å‘Š.txt')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"  ğŸ’¾ è¯„ä¼°æŠ¥å‘Šä¿å­˜è‡³: {report_path}")
    
    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºç®€è¦æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœæ‘˜è¦:")
    print(f"  æ¨¡å‹: {config.model.model_id}")
    print(f"  æ€»æ ·æœ¬: {total_samples}")
    print(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
    print(f"  å¹³å‡åŒ¹é…ç¨‹åº¦: {avg_similarity*100:.1f}%")
    if numerical_stats:
        valid_num_df = df[df['is_numerical']].dropna(subset=['error_percentage'])
        if len(valid_num_df) > 0:
            avg_error = valid_num_df['error_percentage'].mean()
            print(f"  å¹³å‡æ•°å€¼è¯¯å·®: {avg_error:.2f}%")
    print("="*60)


@hydra.main(version_base="1.1", config_path="../configs", config_name="eval_config")
def main(cfg: DictConfig):
    """ä¸»å‡½æ•°"""
    print("=== ğŸ”§ é…ç½®ä¿¡æ¯ ===")
    print(OmegaConf.to_yaml(cfg))
    
    print("ğŸ” å•æ¨¡å‹è¯„ä¼°å·¥å…·")
    print("ä½¿ç”¨åŒé‡è¯„ä¼°æŒ‡æ ‡: 1.æ–‡æœ¬ä¸€è‡´æ€§è®¡ç®— 2.æ•°å€¼è¯¯å·®ç™¾åˆ†æ¯”")
    print("="*80)
    
    # ======================================================================
    # ğŸƒ è¿è¡Œç¯å¢ƒè®¾ç½®
    # ======================================================================
    if hasattr(cfg.run, 'cuda_visible_devices') and cfg.run.cuda_visible_devices is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(cfg.run.cuda_visible_devices)
        print(f"ğŸ”§ è®¾ç½®CUDAè®¾å¤‡: {cfg.run.cuda_visible_devices}")
    
    # è®¾ç½®éšæœºç§å­
    if hasattr(cfg.run, 'seed') and cfg.run.seed is not None:
        set_seed(cfg.run.seed)
        print(f"ğŸ² è®¾ç½®éšæœºç§å­: {cfg.run.seed}")
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        llm, _, model_path = load_model(cfg)
        
        # 2. åŠ è½½æµ‹è¯•æ ·æœ¬
        test_samples = load_test_samples(cfg)
        if not test_samples:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ ·æœ¬")
            return
        
        # 3. æ‰§è¡Œè¯„ä¼°æµ‹è¯•
        results = evaluate_model_on_samples(llm, model_path, test_samples, cfg)
        
        # 4. ä¿å­˜ç»“æœåˆ°CSV
        df = save_results_to_csv(results, cfg)
        
        # 5. ç”Ÿæˆå¯è§†åŒ–
        create_visualization(df, cfg)
        
        print("\nğŸ‰ æ¨¡å‹è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“„ è¯¦ç»†ç»“æœ: {os.path.join(cfg.output.base_output_dir, cfg.output.csv_filename)}")
        print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨: {os.path.join(cfg.output.base_output_dir, cfg.output.visualization_dir)}/")
        print(f"ğŸ“ æµ‹è¯•æ ·æœ¬: {os.path.join(cfg.output.base_output_dir, cfg.output.samples_dir)}/")
        print(f"ğŸ“‹ è¯„ä¼°æŠ¥å‘Š: {os.path.join(cfg.output.base_output_dir, cfg.output.reports_dir)}/")
        print(f"ğŸ—‚ï¸  æ€»è¾“å‡ºç›®å½•: {cfg.output.base_output_dir}/")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()