"""
merge_and_push.py
-----------------
åˆå¹¶ LoRA æƒé‡åˆ°åŸºåº§æ¨¡å‹ï¼Œå¹¶æ¨é€åˆ° Hugging Face Hub.
ä¸“é—¨é’ˆå¯¹ Qwen2.5-VL æ¨¡å‹ä¼˜åŒ–

ç”¨æ³•: python scripts/merge_and_push.py --base unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit \
                               --adapter ./final_model \
                               --repo Litian2002/Qwen2.5-VL-3B-Spatial-bnb-4bit
"""
import argparse, os, json, datetime, textwrap, subprocess
import torch

parser = argparse.ArgumentParser()
parser.add_argument("--base", required=True, help="åŸºåº§æ¨¡å‹ï¼ˆHF hub id æˆ–æœ¬åœ°è·¯å¾„ï¼‰")
parser.add_argument("--adapter", required=True, help="LoRA æƒé‡ç›®å½•")
parser.add_argument("--repo", required=True, help="æƒ³è¦ä¸Šä¼ åˆ° HF çš„ repo_id")
parser.add_argument("--private", action="store_true", help="æ˜¯å¦åˆ›å»ºç§æœ‰ä»“åº“")
parser.add_argument("--dtype", default="fp16", choices=["fp16", "float16", "bf16", "bfloat16", "fp32", "float32", "auto"])
args = parser.parse_args()

out_dir = "./merged_vlm"
os.makedirs(out_dir, exist_ok=True)

print("ğŸš€ å¼€å§‹LoRAæ¨¡å‹åˆå¹¶æµç¨‹...")
print(f"ğŸ“ åŸºåº§æ¨¡å‹: {args.base}")
print(f"ğŸ”— LoRAé€‚é…å™¨: {args.adapter}")
print(f"ğŸ“¤ ç›®æ ‡ä»“åº“: {args.repo}")

# 1ï¸âƒ£ ä½¿ç”¨ unsloth è¿›è¡Œæ¨¡å‹åˆå¹¶ï¼ˆå¯¹Qwen2.5-VLæœ€å¯é ï¼‰
try:
    print("ğŸ”— Loading base model with unsloth...")
    from unsloth import FastVisionModel
    
    # åŠ è½½åŸºåº§æ¨¡å‹
    model, tokenizer = FastVisionModel.from_pretrained(args.base)
    print("âœ… åŸºåº§æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åŠ è½½LoRAé€‚é…å™¨
    print("ğŸ”— Loading LoRA adapter...")
    from peft import PeftModel
    model = PeftModel.from_pretrained(model, args.adapter)
    print("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
    
    # åˆå¹¶æƒé‡
    print("ğŸ“ Merging weights...")
    model = model.merge_and_unload()
    print("âœ… æƒé‡åˆå¹¶å®Œæˆ")
    
    # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼
    FastVisionModel.for_inference(model)
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    print("ğŸ’¾ Saving merged model...")
    model.save_pretrained(out_dir, safe_serialization=True)
    tokenizer.save_pretrained(out_dir)
    print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")
    
except ImportError:
    print("âŒ unsloth ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ transformers...")
    try:
        from transformers import AutoModel, AutoTokenizer
        from peft import PeftModel
        
        # dtypeå­—ç¬¦ä¸²åˆ°torchç±»å‹çš„æ˜ å°„
        DTYPE_MAP = {
            "fp16": torch.float16,
            "float16": torch.float16,
            "bf16": torch.bfloat16,
            "bfloat16": torch.bfloat16,
            "fp32": torch.float32,
            "float32": torch.float32,
        }
        
        if args.dtype == "auto":
            selected_torch_dtype = "auto"
        else:
            selected_torch_dtype = DTYPE_MAP.get(args.dtype.lower())
            if selected_torch_dtype is None:
                raise ValueError(f"ä¸æ”¯æŒçš„dtype: {args.dtype}")
        
        print("ğŸ”— Loading base model with transformers...")
        model = AutoModel.from_pretrained(
            args.base,
            torch_dtype=selected_torch_dtype,
            trust_remote_code=True,
        )
        
        tokenizer = AutoTokenizer.from_pretrained(args.base, trust_remote_code=True)
        print("âœ… åŸºåº§æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        print("ğŸ”— Loading LoRA adapter...")
        model = PeftModel.from_pretrained(model, args.adapter)
        print("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
        
        print("ğŸ“ Merging weights...")
        model = model.merge_and_unload()
        print("âœ… æƒé‡åˆå¹¶å®Œæˆ")
        
        print("ğŸ’¾ Saving merged model...")
        model.save_pretrained(out_dir, safe_serialization=True)
        tokenizer.save_pretrained(out_dir)
        print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ transformers åŠ è½½å¤±è´¥: {e}")
        raise

except Exception as e:
    print(f"âŒ unsloth åŠ è½½å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£… unsloth æˆ–æ›´æ–° transformers ç‰ˆæœ¬")
    raise

# 2ï¸âƒ£ ç”Ÿæˆ README / model card
print("ğŸ“ ç”Ÿæˆæ¨¡å‹å¡ç‰‡...")
readme_path = os.path.join(out_dir, "README.md")
card = textwrap.dedent(f"""
    ---
    license: apache-2.0
    base_model: {args.base}
    merges:
      - adapter: {os.path.basename(args.adapter)}
        method: merge_and_unload
        date: {datetime.date.today()}
    tags:
      - vision-language
      - lora-merged
      - qwen2.5-vl
    ---

    # ğŸ Merged Qwen2.5-VL Model (LoRA + Base)

    This repository contains the **merged** weights of **LoRA adapter** located at `{args.adapter}` and the base
    model **{args.base}**.  
    
    The merge was performed with `peft.merge_and_unload()` on {datetime.date.today()}.

    ## Usage

    ```python
    from unsloth import FastVisionModel
    
    model, tokenizer = FastVisionModel.from_pretrained("{args.repo}")
    model = FastVisionModel.for_inference(model) # Enable native 2x faster inference
    
    # Your inference code here
    ```

    Or with transformers:

    ```python
    from transformers import AutoModel, AutoTokenizer
    
    model = AutoModel.from_pretrained("{args.repo}", trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained("{args.repo}", trust_remote_code=True)
    ```
    """).strip()

with open(readme_path, "w", encoding="utf-8") as f:
    f.write(card + "\n")
print("âœ… æ¨¡å‹å¡ç‰‡ç”Ÿæˆå®Œæˆ")

# 3ï¸âƒ£ Hugging Face Hub ä¸Šä¼ 
print("ğŸ” å‡†å¤‡ä¸Šä¼ åˆ° Hugging Face Hub...")
try:
    from huggingface_hub import HfApi, HfFolder, upload_folder, create_repo
    
    api = HfApi()
    
    # è‹¥æœªç™»å½•ä¼šæç¤ºè¾“å…¥ token
    if HfFolder.get_token() is None:
        print("ğŸ” éœ€è¦ç™»å½• Hugging Face Hubï¼Œä¸€æ¬¡å³å¯ï¼š")
        subprocess.run(["huggingface-cli", "login"])
    
    # åˆ›å»ºä»“åº“ (å­˜åœ¨åˆ™ skip)
    try:
        create_repo(repo_id=args.repo, private=args.private, exist_ok=True)
        print(f"ğŸ“ Repository `{args.repo}` ready.")
    except Exception as e:
        print("âš ï¸ Repo create error(å¯èƒ½å·²å­˜åœ¨)ï¼š", e)
    
    # ä¸Šä¼ ç›®å½•
    print("ğŸš€ Uploading files (this may take a while for >1 GB weights)...")
    upload_folder(
        repo_id=args.repo,
        folder_path=out_dir,
        path_in_repo=".",           # æ ¹ç›®å½•
        commit_message="Upload merged Qwen2.5-VL model",
        ignore_patterns=["*.pt"],   # å¦‚æœä½ åªä¿ç•™ .safetensorsï¼Œå¯ç•¥è¿‡ .pt
    )
    
    print(f"âœ… All done! View: https://huggingface.co/{args.repo}")
    
except ImportError:
    print("âŒ huggingface_hub ä¸å¯ç”¨ï¼Œè·³è¿‡ä¸Šä¼ ")
    print(f"ğŸ’¾ åˆå¹¶çš„æ¨¡å‹å·²ä¿å­˜åˆ°: {out_dir}")
    print("ğŸ“ è¯·æ‰‹åŠ¨ä¸Šä¼ æˆ–å®‰è£… huggingface_hub")
    
except Exception as e:
    print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
    print(f"ğŸ’¾ åˆå¹¶çš„æ¨¡å‹å·²ä¿å­˜åˆ°: {out_dir}")
    print("ğŸ“ ä½ å¯ä»¥æ‰‹åŠ¨ä¸Šä¼ è¿™äº›æ–‡ä»¶")
