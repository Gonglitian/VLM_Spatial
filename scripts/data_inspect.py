#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†æ£€æŸ¥å™¨ - Spatial VLM QA Dataset
ä¸‹è½½å¹¶åˆ†æ https://huggingface.co/datasets/Litian2002/spatialvlm_qa
"""

import os
import matplotlib.pyplot as plt
import numpy as np
from datasets import load_dataset
from PIL import Image
import pandas as pd

def download_and_inspect_dataset():
    """ä¸‹è½½å¹¶æ£€æŸ¥ spatialvlm_qa æ•°æ®é›†"""
    
    print("ğŸ”„ æ­£åœ¨ä¸‹è½½ Spatial VLM QA æ•°æ®é›†...")
    print("ğŸ“ æ•°æ®é›†åœ°å€: https://huggingface.co/datasets/Litian2002/spatialvlm_qa")
    
    try:
        # ä¸‹è½½æ•°æ®é›†
        dataset = load_dataset("Litian2002/spatialvlm_qa")
        print("âœ… æ•°æ®é›†ä¸‹è½½æˆåŠŸ!")
        
        # åŸºæœ¬ä¿¡æ¯
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®é›†åŸºæœ¬ä¿¡æ¯")
        print("="*60)
        print(f"æ•°æ®é›†åç§°: Spatial VLM QA")
        print(f"æ€»æ ·æœ¬æ•°: {len(dataset['train'])}")
        print(f"æ•°æ®åˆ†å‰²: {list(dataset.keys())}")
        
        # æŸ¥çœ‹å­—æ®µç»“æ„
        train_data = dataset['train']
        print(f"\nğŸ“‹ æ•°æ®å­—æ®µ:")
        for key, feature in train_data.features.items():
            print(f"  â€¢ {key}: {feature}")
        
        # éšæœºé‡‡æ ·å‡ ä¸ªä¾‹å­
        print("\n" + "="*60)
        print("ğŸ” éšæœºæ ·æœ¬æ£€æŸ¥ (å‰5ä¸ª)")
        print("="*60)
        
        for i in range(min(5, len(train_data))):
            sample = train_data[i]
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"  é—®é¢˜: {sample['question']}")
            print(f"  ç­”æ¡ˆ: {sample['answer']}")
            print(f"  å›¾åƒå°ºå¯¸: {sample['image'].size}")
        
        # ç»Ÿè®¡åˆ†æ
        print("\n" + "="*60)
        print("ğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡åˆ†æ")
        print("="*60)
        
        print("â³ æ­£åœ¨åˆ†æå…¨éƒ¨æ ·æœ¬ï¼Œè¯·ç¨å€™...")
        
        # åˆ†æå…¨éƒ¨æ ·æœ¬
        questions = [sample['question'] for sample in train_data]
        answers = [sample['answer'] for sample in train_data]
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(questions)} ä¸ªé—®ç­”å¯¹")
        
        q_lengths = [len(q.split()) for q in questions]
        a_lengths = [len(a.split()) for a in answers]
        
        print(f"\né—®é¢˜é•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(q_lengths):.1f} è¯")
        print(f"  æœ€çŸ­: {min(q_lengths)} è¯")
        print(f"  æœ€é•¿: {max(q_lengths)} è¯")
        print(f"  ä¸­ä½æ•°: {np.median(q_lengths):.1f} è¯")
        
        print(f"\nç­”æ¡ˆé•¿åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡é•¿åº¦: {np.mean(a_lengths):.1f} è¯")
        print(f"  æœ€çŸ­: {min(a_lengths)} è¯")
        print(f"  æœ€é•¿: {max(a_lengths)} è¯")
        print(f"  ä¸­ä½æ•°: {np.median(a_lengths):.1f} è¯")
        
        # é—®é¢˜ç±»å‹åˆ†æ
        print(f"\né—®é¢˜ç±»å‹åˆ†æ (å…¨éƒ¨ {len(questions)} ä¸ªæ ·æœ¬):")
        print("â³ æ­£åœ¨åˆ†æé—®é¢˜ç±»å‹...")
        
        question_types = {}
        for i, q in enumerate(questions):
            if i % 5000 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(questions)} ({i/len(questions)*100:.1f}%)")
            
            q_lower = q.lower()
            if any(word in q_lower for word in ['distance', 'how far', 'how distant']):
                question_types['è·ç¦»æµ‹é‡'] = question_types.get('è·ç¦»æµ‹é‡', 0) + 1
            elif any(word in q_lower for word in ['front', 'back', 'behind']):
                question_types['å‰åå…³ç³»'] = question_types.get('å‰åå…³ç³»', 0) + 1
            elif any(word in q_lower for word in ['width', 'height', 'measure', 'size', 'big', 'large', 'small']):
                question_types['å°ºå¯¸æµ‹é‡'] = question_types.get('å°ºå¯¸æµ‹é‡', 0) + 1
            elif any(word in q_lower for word in ['positioned', 'located', 'where']):
                question_types['ä½ç½®å…³ç³»'] = question_types.get('ä½ç½®å…³ç³»', 0) + 1
            elif any(word in q_lower for word in ['color', 'colour']):
                question_types['é¢œè‰²è¯†åˆ«'] = question_types.get('é¢œè‰²è¯†åˆ«', 0) + 1
            elif any(word in q_lower for word in ['count', 'how many', 'number of']):
                question_types['æ•°é‡ç»Ÿè®¡'] = question_types.get('æ•°é‡ç»Ÿè®¡', 0) + 1
            else:
                question_types['å…¶ä»–'] = question_types.get('å…¶ä»–', 0) + 1
        
        print(f"\nâœ… é—®é¢˜ç±»å‹åˆ†æå®Œæˆ:")
        for qtype, count in sorted(question_types.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {qtype}: {count:,} ({count/len(questions)*100:.1f}%)")
        
        # ç­”æ¡ˆç±»å‹åˆ†æ  
        print(f"\nç­”æ¡ˆç±»å‹åˆ†æ:")
        print("â³ æ­£åœ¨åˆ†æç­”æ¡ˆç±»å‹...")
        
        numeric_answers = 0
        yes_no_answers = 0
        color_answers = 0
        other_answers = 0
        
        for i, a in enumerate(answers):
            if i % 5000 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(answers)} ({i/len(answers)*100:.1f}%)")
            
            a_lower = a.lower().strip()
            if a_lower in ['yes', 'no', 'yes.', 'no.']:
                yes_no_answers += 1
            elif any(char.isdigit() for char in a_lower):
                numeric_answers += 1
            elif any(color in a_lower for color in ['red', 'blue', 'green', 'yellow', 'white', 'black', 'orange', 'purple']):
                color_answers += 1
            else:
                other_answers += 1
        
        print(f"\nâœ… ç­”æ¡ˆç±»å‹åˆ†æå®Œæˆ:")
        print(f"  â€¢ æ•°å€¼å‹ç­”æ¡ˆ: {numeric_answers:,} ({numeric_answers/len(answers)*100:.1f}%)")
        print(f"  â€¢ æ˜¯å¦å‹ç­”æ¡ˆ: {yes_no_answers:,} ({yes_no_answers/len(answers)*100:.1f}%)")
        print(f"  â€¢ é¢œè‰²å‹ç­”æ¡ˆ: {color_answers:,} ({color_answers/len(answers)*100:.1f}%)")
        print(f"  â€¢ å…¶ä»–ç­”æ¡ˆ: {other_answers:,} ({other_answers/len(answers)*100:.1f}%)")
        
        # ä¿å­˜å‡ ä¸ªç¤ºä¾‹å›¾åƒ
        print("\n" + "="*60)
        print("ğŸ’¾ ä¿å­˜ç¤ºä¾‹å›¾åƒ")
        print("="*60)
        
        os.makedirs("dataset_samples", exist_ok=True)
        
        for i in range(min(3, len(train_data))):
            sample = train_data[i]
            img = sample['image']
            img.save(f"dataset_samples/sample_{i+1}.png")
            
            # ä¿å­˜é—®ç­”å¯¹
            with open(f"dataset_samples/sample_{i+1}_qa.txt", "w", encoding="utf-8") as f:
                f.write(f"é—®é¢˜: {sample['question']}\n")
                f.write(f"ç­”æ¡ˆ: {sample['answer']}\n")
            
            print(f"  ä¿å­˜æ ·æœ¬ {i+1}: sample_{i+1}.png + sample_{i+1}_qa.txt")
        
        print(f"\nâœ… ç¤ºä¾‹æ–‡ä»¶ä¿å­˜åˆ°: ./dataset_samples/")
        
        # åˆ›å»ºç®€å•çš„å¯è§†åŒ–
        create_visualization(questions, answers, q_lengths, a_lengths, question_types)
        
        return dataset
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return None

def create_visualization(questions, answers, q_lengths, a_lengths, question_types):
    """åˆ›å»ºæ•°æ®é›†ç»Ÿè®¡å¯è§†åŒ–"""
    
    print("\nğŸ“Š åˆ›å»ºç»Ÿè®¡å›¾è¡¨...")
    
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']  # æ”¯æŒä¸­æ–‡
    plt.rcParams['axes.unicode_minus'] = False
    
    fig = plt.figure(figsize=(16, 12))
    
    # é—®é¢˜é•¿åº¦åˆ†å¸ƒ
    ax1 = plt.subplot(2, 3, 1)
    ax1.hist(q_lengths, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.set_title(f'Question Length Distribution\n(Total: {len(questions):,} samples)')
    ax1.set_xlabel('Number of Words')
    ax1.set_ylabel('Frequency')
    ax1.grid(True, alpha=0.3)
    
    # ç­”æ¡ˆé•¿åº¦åˆ†å¸ƒ
    ax2 = plt.subplot(2, 3, 2)
    ax2.hist(a_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')
    ax2.set_title(f'Answer Length Distribution\n(Total: {len(answers):,} samples)')
    ax2.set_xlabel('Number of Words')
    ax2.set_ylabel('Frequency')
    ax2.grid(True, alpha=0.3)
    
    # é—®é¢˜å…³é”®è¯ç»Ÿè®¡
    ax3 = plt.subplot(2, 3, 3)
    keywords = ['distance', 'front', 'back', 'width', 'height', 'measure', 'far', 'color', 'count']
    keyword_counts = []
    for keyword in keywords:
        count = sum(1 for q in questions if keyword in q.lower())
        keyword_counts.append(count)
    
    ax3.bar(keywords, keyword_counts, color='lightgreen', alpha=0.7)
    ax3.set_title('Question Keywords Frequency')
    ax3.set_xlabel('Keywords')
    ax3.set_ylabel('Count')
    ax3.tick_params(axis='x', rotation=45)
    
    # ç­”æ¡ˆç±»å‹é¥¼å›¾
    ax4 = plt.subplot(2, 3, 4)
    yes_count = sum(1 for a in answers if a.lower().strip() in ['yes', 'yes.'])
    no_count = sum(1 for a in answers if a.lower().strip() in ['no', 'no.'])
    numeric_count = sum(1 for a in answers if any(c.isdigit() for c in a) and a.lower().strip() not in ['yes', 'no', 'yes.', 'no.'])
    color_count = sum(1 for a in answers if any(color in a.lower() for color in ['red', 'blue', 'green', 'yellow', 'white', 'black', 'orange', 'purple']) and not any(c.isdigit() for c in a))
    other_count = len(answers) - yes_count - no_count - numeric_count - color_count
    
    labels = ['Yes', 'No', 'Numeric', 'Color', 'Other']
    sizes = [yes_count, no_count, numeric_count, color_count, other_count]
    colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightyellow', 'lightpink']
    
    ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    ax4.set_title('Answer Type Distribution')
    
    # é—®é¢˜ç±»å‹åˆ†å¸ƒ
    ax5 = plt.subplot(2, 3, 5)
    qtypes = list(question_types.keys())
    qcounts = list(question_types.values())
    
    ax5.bar(qtypes, qcounts, color='lightsteelblue', alpha=0.7)
    ax5.set_title('Question Type Distribution')
    ax5.set_xlabel('Question Types')
    ax5.set_ylabel('Count')
    ax5.tick_params(axis='x', rotation=45)
    
    # é•¿åº¦åˆ†å¸ƒå¯¹æ¯”
    ax6 = plt.subplot(2, 3, 6)
    ax6.scatter(q_lengths, a_lengths, alpha=0.3, s=1)
    ax6.set_xlabel('Question Length (words)')
    ax6.set_ylabel('Answer Length (words)')
    ax6.set_title('Question vs Answer Length')
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('dataset_analysis.png', dpi=300, bbox_inches='tight')
    print("  ğŸ“Š ç»Ÿè®¡å›¾è¡¨ä¿å­˜ä¸º: dataset_analysis.png")
    
    # ä¿å­˜è¯¦ç»†ç»Ÿè®¡åˆ°æ–‡ä»¶
    with open('dataset_statistics.txt', 'w', encoding='utf-8') as f:
        f.write("Spatial VLM QA Dataset - å®Œæ•´ç»Ÿè®¡åˆ†æ\n")
        f.write("="*60 + "\n\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(questions):,}\n\n")
        
        f.write("é—®é¢˜é•¿åº¦ç»Ÿè®¡:\n")
        f.write(f"  å¹³å‡é•¿åº¦: {np.mean(q_lengths):.2f} è¯\n")
        f.write(f"  ä¸­ä½æ•°: {np.median(q_lengths):.2f} è¯\n")
        f.write(f"  æ ‡å‡†å·®: {np.std(q_lengths):.2f}\n")
        f.write(f"  æœ€çŸ­: {min(q_lengths)} è¯\n")
        f.write(f"  æœ€é•¿: {max(q_lengths)} è¯\n\n")
        
        f.write("ç­”æ¡ˆé•¿åº¦ç»Ÿè®¡:\n")
        f.write(f"  å¹³å‡é•¿åº¦: {np.mean(a_lengths):.2f} è¯\n")
        f.write(f"  ä¸­ä½æ•°: {np.median(a_lengths):.2f} è¯\n")
        f.write(f"  æ ‡å‡†å·®: {np.std(a_lengths):.2f}\n")
        f.write(f"  æœ€çŸ­: {min(a_lengths)} è¯\n")
        f.write(f"  æœ€é•¿: {max(a_lengths)} è¯\n\n")
        
        f.write("é—®é¢˜ç±»å‹åˆ†å¸ƒ:\n")
        for qtype, count in sorted(question_types.items(), key=lambda x: x[1], reverse=True):
            f.write(f"  {qtype}: {count:,} ({count/len(questions)*100:.2f}%)\n")
        
        f.write(f"\nç­”æ¡ˆç±»å‹åˆ†å¸ƒ:\n")
        f.write(f"  æ•°å€¼å‹: {numeric_count:,} ({numeric_count/len(answers)*100:.2f}%)\n")
        f.write(f"  æ˜¯å¦å‹: {yes_count + no_count:,} ({(yes_count + no_count)/len(answers)*100:.2f}%)\n")
        f.write(f"  é¢œè‰²å‹: {color_count:,} ({color_count/len(answers)*100:.2f}%)\n")
        f.write(f"  å…¶ä»–: {other_count:,} ({other_count/len(answers)*100:.2f}%)\n")
    
    print("  ğŸ“„ è¯¦ç»†ç»Ÿè®¡ä¿å­˜ä¸º: dataset_statistics.txt")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Spatial VLM QA æ•°æ®é›†æ£€æŸ¥å™¨ (å…¨é‡åˆ†æ)")
    print("=" * 60)
    
    dataset = download_and_inspect_dataset()
    
    if dataset:
        print("\nğŸ‰ æ•°æ®é›†æ£€æŸ¥å®Œæˆ!")
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  â€¢ dataset_samples/ - ç¤ºä¾‹å›¾åƒå’Œé—®ç­”å¯¹")
        print("  â€¢ dataset_analysis.png - ç»Ÿè®¡åˆ†æå›¾è¡¨")
        print("  â€¢ dataset_statistics.txt - è¯¦ç»†ç»Ÿè®¡æ•°æ®")
        
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("  1. è¿™æ˜¯ä¸€ä¸ªåˆæˆçš„3Dç©ºé—´æ¨ç†æ•°æ®é›†")
        print("  2. åŒ…å«çº¦40kä¸ªå›¾åƒ-é—®ç­”å¯¹")
        print("  3. ä¸»è¦ç”¨äºè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›")
        print("  4. é—®é¢˜ç±»å‹æ¶µç›–è·ç¦»æµ‹é‡ã€ä½ç½®å…³ç³»ã€å°ºå¯¸ç­‰")
        print("  5. å·²å®Œæˆå…¨é‡æ ·æœ¬åˆ†æï¼Œç»“æœæ›´å‡†ç¡®å…¨é¢")
        
    else:
        print("\nâŒ æ•°æ®é›†æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–åº“")

if __name__ == "__main__":
    main()
